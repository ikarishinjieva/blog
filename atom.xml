<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tac say]]></title>
  <link href="http://ikarishinjieva.github.com/blog/atom.xml" rel="self"/>
  <link href="http://ikarishinjieva.github.com/blog/"/>
  <updated>2014-04-04T20:38:37+08:00</updated>
  <id>http://ikarishinjieva.github.com/blog/</id>
  <author>
    <name><![CDATA[Tac Huang (ikari_shinji@github)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[整理一下最近读的MDL源码]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/04/04/MDL/"/>
    <updated>2014-04-04T20:00:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/04/04/MDL</id>
    <content type="html"><![CDATA[<p>以下都是个人理解, 如有疏漏请斧正
另, 因为理解不深, 将忽略锁级别以及锁共享的细节</p>

<h2>MDL</h2>

<p>MDL (Metadata lock), 除了正常的Condition var提供的功能外, 还额外提供了
1. 不同的锁级别. 在不冲突的情况下, 允许共享资源
2. 死锁检查和处理
3. 记录等待状态, 是死锁检查的基础</p>

<h2>模型</h2>

<p><code>MDL_lock</code> 表示Mysqld中的一个资源(库/表/&#8230;) 存储在全局结构 <code>mdl_locks (MDL_map)</code>中, <code>mdl_locks</code>内有<code>m_partitions</code> (锁的分区), 用来分散查找lock时的竞争</p>

<p><code>MDL_context</code> 为MDL上下文接口, 表示一个资源竞争者, THD实现了这个接口, 即一个Mysqld的线程可以是<code>MDL_lock</code>的资源竞争者</p>

<p><code>MDL_ticket</code> 表示<code>MDL_lock</code>的许可或请求, 会同时挂在两处:</p>

<ol>
<li>挂在所属<code>MDL_Context</code>中, 通过<code>MDL_ticket.next_in_context/prev_in_context</code>组织链表</li>
<li>挂在<code>MDL_lock</code>的队列中, 通过<code>MDL_ticket.next_in_lock/prev_in_lock</code>组织链表. <code>MDL_lock</code>的队列分为两种, 一个<code>MDL_ticket</code>可能会挂在其中之一

<ul>
<li>挂在<code>MDL_lock</code>的等待队列(<code>MDL_lock.m_waiting</code>)中, 表示<code>MDL_ticket</code>的owner (<code>MDL_context</code>)正在等待该资源(<code>MDL_lock</code>)</li>
<li>挂在<code>MDL_lock</code>的已许可队列(<code>MDL_lock.m_granted</code>)中, 表示<code>MDL_ticket</code>的owner (<code>MDL_context</code>)已经获得该资源(<code>MDL_lock</code>)</li>
</ul>
</li>
</ol>


<p>总结一下, <code>MDL_context</code>和<code>MDL_ticket</code>的关系是一对多, 一个竞争者可以同时申请/获得多个资源的许可; <code>MDL_ticket</code>和<code>MDL_lock</code>的关系是多对一, 可以同时有多个资源许可在竞争一个资源, 或者多个资源许可可以<em>有条件</em>地共享一个资源</p>

<h2>如何获得锁</h2>

<p>简单分析<code>MDL_context::acquire_lock</code>方法, 其主要流程是</p>

<pre><code>bool MDL_context::acquire_lock(MDL_request *mdl_request, ulong lock_wait_timeout) {
    ...

    try_acquire_lock_impl(...) 
    //尝试不等待立刻获得资源, 如果成功直接返回
    //以下是等待资源的处理
    ...
    lock-&gt;m_waiting.add_ticket(ticket) 
    //将一个资源申请`ticket`挂入资源`lock`的等待队列`m_waiting`
    if (lock-&gt;needs_notification(ticket)) {
        //如果等待资源时需要通知状态, 则不断轮询并通知
        //将忽略此处的细节
        ...
    } else {
        //等待资源
        //结果可能是获得资源, 或者超时, 或者异常 (比如被死锁检测机制判定死亡)
        //`timed_wait`中的实现是等待COND(条件变量)`m_wait.m_COND_wait_status`
        wait_status= m_wait.timed_wait(...);
    }
    //收尾处理
    m_tickets[mdl_request-&gt;duration].push_front(ticket)
    //将资源申请`ticket`挂入`MDL_Context.m_tickets`
    ...
}
</code></pre>

<h3>记录等待状态</h3>

<p>之前提到了记录等待状态, 在<code>MDL_context::acquire_lock</code>方法中可以看到如下代码 (上一节未列出)</p>

<pre><code>bool MDL_context::acquire_lock(MDL_request *mdl_request, ulong lock_wait_timeout) {
    m_wait.reset_status();
    ...
    will_wait_for(ticket); //其中设置了`m_waiting_for`
    if (lock-&gt;needs_notification(ticket)) {
        ...
        //等待资源
        wait_status= m_wait.timed_wait(m_owner, &amp;abs_timeout, TRUE,
                                  mdl_request-&gt;key.get_wait_state_name());
    } else {
        //等待资源
        wait_status= m_wait.timed_wait(m_owner, &amp;abs_timeout, TRUE,
                                  mdl_request-&gt;key.get_wait_state_name());
    }
    done_waiting_for(); //其中清空了`m_waiting_for`
    ...
}
</code></pre>

<p>可以看到<code>MDL_context.m_wait</code>是用来等待资源的工具类, 其中进行等待处理, 并记录等待资源的状态/结果.</p>

<p>还有一个<code>MDL_context.m_waiting_for</code>也在记录<code>MDL_context</code>正在进行的资源申请(<code>MDL_ticket</code>), 其正在等待某个资源. 实际上<code>m_waiting_for</code>是冗余的信息, 至于原因源代码中有解释, 此处不冗余说明&#8230;</p>

<h2>如何释放锁</h2>

<p>释放锁, 需要完成下面几个动作:</p>

<ol>
<li>将<code>ticket</code>从<code>MDL_lock</code>的数据结构上卸下来</li>
<li>调度选择新的锁占有者</li>
<li>将<code>ticket</code>从<code>MDL_context</code>的数据结构上卸下并回收</li>
</ol>


<p>入口为<code>MDL_context::release_lock</code></p>

<pre><code>void MDL_context::release_lock(enum_mdl_duration duration, MDL_ticket *ticket) 
{
    ...
    lock-&gt;remove_ticket(&amp;MDL_lock::m_granted, ticket) {
        //将`ticket`从`MDL_lock`的数据结构上卸下来
        (this-&gt;*list).remove_ticket(ticket);
        ...
        //调度选择新的锁占有者
        reschedule_waiters();
    }()

    //将`ticket`从`MDL_context`的数据结构上卸下并回收
    m_tickets[duration].remove(ticket);
    MDL_ticket::destroy(ticket);
    ...
}
</code></pre>

<p>下面说明调度的细节</p>

<h3>释放锁时的调度</h3>

<p>调度函数的入口是<code>MDL_lock::reschedule_waiters</code></p>

<p>最简单的调度就是从<code>MDL_lock.m_waiting</code>队列中取出头元素, 直接将资源调度给头元素即可</p>

<p>Mysqld在此基础上添加了一个退让条件:
如果资源连续被<em>高优先级</em>(比如<code>SNW</code>/<code>SNRW</code>/<code>X</code>锁类型)的<code>ticket</code>获得, 那么退让一步, 允许资源间隔被调度给<em>低优先级</em>的<code>ticket</code>防止其饿死.</p>

<p>用<code>MDL_lock::reschedule_waiters</code>的代码说就是, 如果<code>MDL_lock</code>被连续分配给<code>hog_lock_types_bitmap()</code>中定义的<em>高优先级</em>类型的<code>ticket</code>,连续的次数<code>m_hog_lock_count</code>超过<code>max_write_lock_count</code>, 那么开启退让条件, 批准第一个<em>非</em><em>高优先级</em>的<code>ticket</code>获得资源</p>

<h2>死锁检测</h2>

<p>死锁检测的入口是<code>MDL_context::find_deadlock</code>, 本身原理很简单, 但源码写的很复杂= =. 先说明原理, 再对应源码</p>

<p>设当前<code>MDL_context</code>为图的一个节点<code>A</code>, 从节点<code>A</code>出发,  找到<code>A</code>的正在等待的资源<code>L</code>(<code>A.m_waiting_for.m_lock</code>)中的<code>m_granted</code>里的每一个<code>MDL_ticket</code>对应的<code>MDL_context</code> <code>B</code>, 表示<code>A</code>正在等待<code>B</code>释放资源<code>L</code>. 在图中<code>A</code> -> <code>B</code> 添加一条有向边</p>

<p>死锁检查的工作就是遍历这张有向图, 检查其是否存在环路</p>

<p>以<code>MDL_context::find_deadlock</code>入口, 展开一些调用来说明代码</p>

<pre><code>(MDL_context::find_deadlock)
while(1) {
    visit_subgraph(visitor) {
        m_waiting_for-&gt;accept_visitor(visitor) {
            m_lock-&gt;visit_subgraph(this, visitor) {
                ...
            }()
        }()
    }()
    break if no deadlock
    set deadlock victim
    break if deadlock victim is current context
}
</code></pre>

<p>可以看到<code>find_deadlock</code>以<code>MDL_context.m_waiting_for.m_lock</code>为起始点, 不断遍历其有向图, 选出victim. 直到
* 没有发现死锁
* 或自己被选为victim</p>

<p>其使用一个visitor (<code>MDL_wait_for_graph_visitor</code>) 贯穿遍历过程, 其记录了遍历的过程</p>

<p>再来看<code>MDL_lock::visit_subgraph</code>, 此函数是以一个<code>MDL_lock</code>为起点, 来遍历依赖图</p>

<pre><code>MDL_lock::visit_subgraph(MDL_ticket *waiting_ticket, MDL_wait_for_graph_visitor *gvisitor) {

    //此处是因为MDL_context.m_waiting_for是冗余信息, 但无法保证更新同步, 带来的额外操作. 忽略此处细节
    if (src_ctx-&gt;m_wait.get_status() != MDL_wait::EMPTY) {...}

    //visitor用来记录遍历层次
    //当遍历层次大于MAX_SEARCH_DEPTH(32), 也认为发现死锁
    if (gvisitor-&gt;enter_node(src_ctx)) {...}

    //由于现在是以一个资源(`MDL_lock`)为视角, 之后的检查为了效率, 遍历会从两个方向同时进行, 即检查节点的出度方向(`MDL_lock.m_granted`)和节点的入度方向(`MDL_lock.m_waiting`). 


    //为了效率, 死锁检测会先检测距离为1的临近节点, 而先不深度遍历图

    while ((ticket= granted_it++))
    {
      if (ticket-&gt;get_ctx() != src_ctx &amp;&amp;
          ticket-&gt;is_incompatible_when_granted(waiting_ticket-&gt;get_type()) &amp;&amp;
          gvisitor-&gt;inspect_edge(ticket-&gt;get_ctx()))
      {
        goto end_leave_node;
      }
    }

    while ((ticket= waiting_it++))
    {
      /* Filter out edges that point to the same node. */
      if (ticket-&gt;get_ctx() != src_ctx &amp;&amp;
          ticket-&gt;is_incompatible_when_waiting(waiting_ticket-&gt;get_type()) &amp;&amp;
          gvisitor-&gt;inspect_edge(ticket-&gt;get_ctx()))
      {
        goto end_leave_node;
      }
    }

    //此处开始, 深度遍历图

    granted_it.rewind();
    while ((ticket= granted_it++))
    {
      if (ticket-&gt;get_ctx() != src_ctx &amp;&amp;
          ticket-&gt;is_incompatible_when_granted(waiting_ticket-&gt;get_type()) &amp;&amp;
          ticket-&gt;get_ctx()-&gt;visit_subgraph(gvisitor))
      {
        goto end_leave_node;
      }
    }

    waiting_it.rewind();
    while ((ticket= waiting_it++))
    {
      if (ticket-&gt;get_ctx() != src_ctx &amp;&amp;
          ticket-&gt;is_incompatible_when_waiting(waiting_ticket-&gt;get_type()) &amp;&amp;
          ticket-&gt;get_ctx()-&gt;visit_subgraph(gvisitor))
      {
        goto end_leave_node;
      }
    }
    ...

    //visitor退栈
    gvisitor-&gt;leave_node(src_ctx);
    ...
}
</code></pre>

<p>发现死锁后, 会调用<code>Deadlock_detection_visitor::opt_change_victim_to</code>, 其中进行<code>MDL_context</code>权重比较, 来选取一个作为victim, 此处忽略细节</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对Mysql bug #70307 的再学习]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/04/01/study-mysql-bug-70307-2/"/>
    <updated>2014-04-01T13:07:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/04/01/study-mysql-bug-70307-2</id>
    <content type="html"><![CDATA[<p>之前对bug #70307有过<a href="http://ikarishinjieva.github.io/blog/blog/2013/10/25/study-mysql-bug-70307/">学习</a>, 苦于阿兹海默状态, 又花了半天在mysql 5.5.33上探查这个场景的原因&#8230;</p>

<p>简单记录一下</p>

<h4>现象</h4>

<p>mysql进行主从复制, 从机上<code>FLUSH TABLES WITH READ LOCK</code>后, 进行<code>STOP SLAVE</code>, 一定概率下 <code>SHOW SLAVE STATUS</code>卡住</p>

<h4>重现步骤</h4>

<table>
<thead>
<tr>
<th>master </th>
<th> slave client 1 </th>
<th> slave client 2</th>
</tr>
</thead>
<tbody>
<tr>
<td> - </td>
<td> STOP SLAVE IO_THREAD </td>
<td> -</td>
</tr>
<tr>
<td> CREATE TABLE TEST.TEST &#8230; </td>
<td> - </td>
<td> -</td>
</tr>
<tr>
<td> - </td>
<td> FLUSH TABLES WITH READ LOCK </td>
<td> -</td>
</tr>
<tr>
<td> - </td>
<td> START SLAVE IO_THREAD </td>
<td> -</td>
</tr>
<tr>
<td> - </td>
<td> - </td>
<td> STOP SLAVE</td>
</tr>
<tr>
<td> - </td>
<td> SHOW SLAVE STATUS </td>
<td> -</td>
</tr>
</tbody>
</table>


<p>其中, <code>START/STOP SLAVE IO_THREAD</code>是为了在<code>FLUSH TABLES WITH READ LOCK</code>时造成slave io_thread有未提交数据</p>

<h4>死锁原因</h4>

<ol>
<li><code>FLUSH TABLES WITH READ LOCK</code> 会阻塞IO_THREAD提交数据</li>
<li><code>STOP SLAVE</code>会等待IO_THREAD结束 (<code>mi-&gt;stop_cond</code>), 即<code>STOP SLAVE</code>间接被<code>FLUSH TABLES WITH READ LOCK</code>阻塞</li>
<li><code>STOP SLAVE</code>在被阻塞前, 持有了<code>LOCK_active_mi</code>, 独占了<code>master_info</code></li>
<li><code>SHOW SLAVE STATUS</code>会申请锁<code>LOCK_active_mi</code>, 被<code>STOP SLAVE</code>阻塞</li>
<li>如果<code>SHOW SLAVE STATUS</code>是由之前<code>FLUSH TABLES WITH READ LOCK</code>的<code>slave client 1</code>发出的, 那逻辑上相当于自己在等待自己释放资源</li>
<li>从另外的client上<code>UNLOCK TABLES</code>也解不开</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mysql, 利用假master重放binlog]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/03/26/mysql-fake-master-server/"/>
    <updated>2014-03-26T20:08:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/03/26/mysql-fake-master-server</id>
    <content type="html"><![CDATA[<h4>场景</h4>

<p>这次想解决的场景是想在一个mysqld实例上重放一些来自于其他实例的binlog, 传统的方法是<code>mysqlbinlog</code>. 但是<code>mysqlbinlog</code>会带来一些问题, 比如这个<a href="http://bugs.mysql.com/bug.php?id=33048">bug</a></p>

<p>后同事转给我一种利用<a href="http://www.orczhou.com/index.php/2013/11/use-mysql-replication-to-recove-binlog/">复制重放binlog的方法</a>, 其中提到两种方式:</p>

<ul>
<li>第一种是修改relay log的信息, 将binlog作为relay log来放. 这是种很好的方法, 缺点是<code>mysqld</code>需要停机重启. 如果不重启, server中对于<code>relay-log.index</code>和<code>relay-log.info</code>等的缓存不会刷新.</li>
<li>第二种是起另外一个mysqld实例, 将binlog作为relay log, 再将此实例作为master, 向目标实例进行复制. 这种方式的缺点是作为中间人的mysqld实例需要消耗资源</li>
</ul>


<p>于是想办法将第二种方法进行改进, 即制造一个假的master, 实现简单的复制协议, 直接将binlog复制给目标mysqld实例. 与第二种方式相比, 好处在于只使用少量资源 (一个端口, 一点用来读文件的内存).</p>

<h4>实现</h4>

<p>实现参看我的<a href="https://github.com/ikarishinjieva/mysql_binlog_utils/blob/master/fake_master_server.go">github</a></p>

<p><strong>注意: 此实现仅适用于mysql 5.5.33, 其它版本未测试</strong></p>

<p>由于<a href="http://dev.mysql.com/doc/internals/en/client-server-protocol.html">mysql internals</a> 已经将mysql的网络协议写的比较详细, 需要做的只是起一个tcp的server, 同目标mysqld实例进行交互即可.</p>

<p>此处逐层介绍实现, 将忽略不需要特别注意的部分. 为了简单, 将binlog的来源mysqld实例称为A, 目标mysqld实例称为B, 假master称为T.</p>

<p>目标就是讲从A获得的binlog文件, 通过T, 在B上重放出来</p>

<p>从B发起<code>start slave</code>, 到T真正向B复制数据, 需要下面两个阶段</p>

<h5>1. Handshake Phase</h5>

<h5>2. Replication Phase</h5>

<p>先介绍Handshake Phase, 有以下步骤</p>

<h5>1.1 B执行<code>start slave</code>, 此时B向T建立一个TCP连接</h5>

<h5>1.2 T向B发送handshake packet</h5>

<h5>1.3 B向T回复handshake packet response</h5>

<h5>1.4 T向B发送ok packet</h5>

<p>在Replication Phase, 有以下步骤</p>

<h5>2.1 B向T查询<code>SELECT UNIX_TIMESTAMP()</code></h5>

<h5>2.2 B向T查询<code>SHOW VARIABLES LIKE 'SERVER_ID'</code></h5>

<h5>2.3 B向T执行<code>SET @master_heartbeat_period=</code></h5>

<h5>2.4 B向T发送COM_REGISTER_SLAVE packet, 得到T回复的ok packet</h5>

<h5>2.5 B向T发送COM_BINLOG_DUMP packet, T开始向B逐一发送binlog event packet</h5>

<p>到目前为止, 所有的packet定义都可以在<a href="http://dev.mysql.com/doc/internals/en/client-server-protocol.html">mysql internals</a>, 逐一实现即可. 这里只简述一些处理packet时需要注意的细节.</p>

<h4>处理packet时需要注意的细节</h4>

<ul>
<li>所有的packet都会包装一个<a href="http://dev.mysql.com/doc/internals/en/mysql-packet.html">header</a>, 其中包括packet payload(不包括header)的大小, 和序号</li>
<li>对于序号的处理, 比如2.2中B向T查询<code>SHOW VARIABLES LIKE 'SERVER_ID'</code>, B向T发送的第一个包序号为0, T向B回复的几个包序号依次递增为1,2,3&#8230;</li>
<li>注意数据类型, 仅整数, mysql的协议里有<a href="http://dev.mysql.com/doc/internals/en/integer.html">定长整数</a>和变长整数(length encoded integer), 需要特别留意packet payload的类型描述</li>
<li>说明一下<a href="http://dev.mysql.com/doc/internals/en/com-query-response.html#packet-COM_QUERY_Response">query response packet</a>. 比如B向T做一个查询, T将通过query response packet来返回查询结果. 需要说明的是, 如果查询结果为空 (比如<code>SET @master_heartbeat_period= ?</code>的结果), 仅需返回<code>COM_QUERY_RESPONSE</code>, 后面不需要跟着空的column定义和row数据</li>
</ul>


<h4>对超大packet的支持</h4>

<p>当一个packet过大 (超过<code>1&lt;&lt;24-1</code>byte ~= 16 MB) 时, 传输需要对packet进行切割, 参看<a href="http://dev.mysql.com/doc/internals/en/sending-more-than-16mbyte.html">这里</a></p>

<p>注意, 在A上生成binlog时, 是可以容纳大于16MB的packet的, 也就是原binlog里存在超大的event, 需要在传输时加以限制</p>

<p>切割packet没什么特别之处, 仅需要注意包格式, 一个20MB的event的传输packet格式举例为 (此处用<code>16MB</code>便于描述, 应为<code>1&lt;&lt;24-1</code>byte):</p>

<pre><code>packet 1
    4字节 packet header
    1字节 值为[00], 是binlog event的特征标志
    16MB-1字节 为第一段数据

packet 2
    4字节 packet header
    20MB-16MB+1字节 为第二段数据
</code></pre>

<p>需要注意的是之后的packet时不带有[00]特征位的. 而包的大小计算范围为<strong>除去前4字节</strong>的全部字节</p>

<h4>一些资料</h4>

<p>除上文提到的资料, 还推荐<a href="http://boytnt.blog.51cto.com/966121/1279318">MySQL通讯协议研究系列</a>, 会对包格式有个直观感觉</p>

<h4>Trouble shooting</h4>

<p>在整个过程中, 有时候需要<code>gdb</code>到<code>mysqld</code>里来了解通讯协议的工作机制, 这里记录几个常用的函数入口点</p>

<h5>1. slave连接到master时</h5>

<pre><code>#0  wait_for_data (fd=21, timeout=3600) at /vagrant/mysql-5.5.35/sql-common/client.c:208
#1  0x00000000007316aa in my_connect (fd=21, name=0x7fa074004fd0, namelen=16, timeout=3600) at /vagrant/mysql-5.5.35/sql-common/client.c:187
#2  0x00000000007363cb in mysql_real_connect (mysql=0x7fa074004960, host=0x3959cc8 "192.168.56.1", user=0x3959d05 "repl", passwd=0x3959d36 "", db=0x0, port=3306, unix_socket=0x0, client_flag=2147483648)
    at /vagrant/mysql-5.5.35/sql-common/client.c:3282
#3  0x000000000057f138 in connect_to_master (thd=0x7fa074000a40, mysql=0x7fa074004960, mi=0x3959640, reconnect=false, suppress_warnings=false) at /vagrant/mysql-5.5.35/sql/slave.cc:4297
#4  0x000000000057edd1 in safe_connect (thd=0x7fa074000a40, mysql=0x7fa074004960, mi=0x3959640) at /vagrant/mysql-5.5.35/sql/slave.cc:4233
#5  0x000000000057b15c in handle_slave_io (arg=0x3959640) at /vagrant/mysql-5.5.35/sql/slave.cc:2851
#6  0x00007fa096751851 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa0954a690d in clone () from /lib64/libc.so.6
</code></pre>

<h5>2. handshake phase</h5>

<pre><code>#0  send_server_handshake_packet (mpvio=0x7fa0942eb450, data=0x391e5b4 "=!-\\gq$\\%&gt;J8z}'EgVW5", data_len=21) at /vagrant/mysql-5.5.35/sql/sql_acl.cc:8084
#1  0x000000000059a87c in server_mpvio_write_packet (param=0x7fa0942eb450, packet=0x391e5b4 "=!-\\gq$\\%&gt;J8z}'EgVW5", packet_len=21) at /vagrant/mysql-5.5.35/sql/sql_acl.cc:9082
#2  0x000000000059bc99 in native_password_authenticate (vio=0x7fa0942eb450, info=0x7fa0942eb468) at /vagrant/mysql-5.5.35/sql/sql_acl.cc:9713
#3  0x000000000059ad86 in do_auth_once (thd=0x391cc70, auth_plugin_name=0x1026760, mpvio=0x7fa0942eb450) at /vagrant/mysql-5.5.35/sql/sql_acl.cc:9336
#4  0x000000000059b23a in acl_authenticate (thd=0x391cc70, connect_errors=0, com_change_user_pkt_len=0) at /vagrant/mysql-5.5.35/sql/sql_acl.cc:9472
#5  0x00000000006d9eb5 in check_connection (thd=0x391cc70) at /vagrant/mysql-5.5.35/sql/sql_connect.cc:575
#6  0x00000000006d9ffc in login_connection (thd=0x391cc70) at /vagrant/mysql-5.5.35/sql/sql_connect.cc:633
#7  0x00000000006da5ba in thd_prepare_connection (thd=0x391cc70) at /vagrant/mysql-5.5.35/sql/sql_connect.cc:789
#8  0x00000000006daa28 in do_handle_one_connection (thd_arg=0x391cc70) at /vagrant/mysql-5.5.35/sql/sql_connect.cc:855
#9  0x00000000006da583 in handle_one_connection (arg=0x391cc70) at /vagrant/mysql-5.5.35/sql/sql_connect.cc:781
#10 0x00007fa096751851 in start_thread () from /lib64/libpthread.so.0
#11 0x00007fa0954a690d in clone () from /lib64/libc.so.6
</code></pre>

<h5>3. query时回复column定义</h5>

<pre><code>#0  Protocol::send_result_set_metadata (this=0x3767610, list=0x3769328, flags=5)
    at /vagrant/mysql-5.5.35/sql/protocol.cc:677
#1  0x00000000005c6745 in select_send::send_result_set_metadata (this=0x7f350c001658, list=..., flags=5)
    at /vagrant/mysql-5.5.35/sql/sql_class.cc:2132
#2  0x000000000062895a in JOIN::exec (this=0x7f350c001678) at /vagrant/mysql-5.5.35/sql/sql_select.cc:1858
#3  0x000000000062b2a0 in mysql_select (thd=0x37670e0, rref_pointer_array=0x3769400, tables=0x0, wild_num=0,
    fields=..., conds=0x0, og_num=0, order=0x0, group=0x0, having=0x0, proc_param=0x0, select_options=2147748608,
    result=0x7f350c001658, unit=0x3768bf8, select_lex=0x3769218) at /vagrant/mysql-5.5.35/sql/sql_select.cc:2604
#4  0x00000000006232f5 in handle_select (thd=0x37670e0, lex=0x3768b48, result=0x7f350c001658,
    setup_tables_done_option=0) at /vagrant/mysql-5.5.35/sql/sql_select.cc:297
#5  0x00000000005fe82d in execute_sqlcom_select (thd=0x37670e0, all_tables=0x0)
    at /vagrant/mysql-5.5.35/sql/sql_parse.cc:4627
#6  0x00000000005f7379 in mysql_execute_command (thd=0x37670e0) at /vagrant/mysql-5.5.35/sql/sql_parse.cc:2178
#7  0x0000000000600a43 in mysql_parse (thd=0x37670e0, rawbuf=0x7f350c001430 "SELECT UNIX_TIMESTAMP()", length=23,
    parser_state=0x7f35195056f0) at /vagrant/mysql-5.5.35/sql/sql_parse.cc:5664
#8  0x00000000005f490a in dispatch_command (command=COM_QUERY, thd=0x37670e0,
    packet=0x3770e21 "SELECT UNIX_TIMESTAMP()", packet_length=23) at /vagrant/mysql-5.5.35/sql/sql_parse.cc:1040
#9  0x00000000005f3c00 in do_command (thd=0x37670e0) at /vagrant/mysql-5.5.35/sql/sql_parse.cc:773
#10 0x00000000006daa4b in do_handle_one_connection (thd_arg=0x37670e0)
    at /vagrant/mysql-5.5.35/sql/sql_connect.cc:862
#11 0x00000000006da583 in handle_one_connection (arg=0x37670e0) at /vagrant/mysql-5.5.35/sql/sql_connect.cc:781
#12 0x00007f352e043851 in start_thread () from /lib64/libpthread.so.0
#13 0x00007f352cd9890d in clone () from /lib64/libc.so.6
</code></pre>

<h5>4. query读取数据结果</h5>

<pre><code>#0  cli_read_query_result (mysql=0x7f3508004960) at /vagrant/mysql-5.5.35/sql-common/client.c:3829
#1  0x0000000000738016 in mysql_real_query (mysql=0x7f3508004960, query=0xb80e34 "SELECT UNIX_TIMESTAMP()",
    length=23) at /vagrant/mysql-5.5.35/sql-common/client.c:3918
#2  0x00000000005766ec in get_master_version_and_clock (mysql=0x7f3508004960, mi=0x375b400)
    at /vagrant/mysql-5.5.35/sql/slave.cc:1328
#3  0x000000000057b35a in handle_slave_io (arg=0x375b400) at /vagrant/mysql-5.5.35/sql/slave.cc:2881
#4  0x00007f352e043851 in start_thread () from /lib64/libpthread.so.0
#5  0x00007f352cd9890d in clone () from /lib64/libc.so.6
</code></pre>

<h5>5. slave发送COM_BINLOG_DUMP</h5>

<pre><code>#0  request_dump (thd=0x7f35f80008c0, mysql=0x7f35f80076c0, mi=0x3301ac0,
    suppress_warnings=0x7f361c189e2b)
    at /vagrant/mysql-5.5.35/sql/slave.cc:2184
#1  0x000000000057b596 in handle_slave_io (arg=0x3301ac0)
    at /vagrant/mysql-5.5.35/sql/slave.cc:2935
#2  0x00007f3620c66851 in start_thread () from /lib64/libpthread.so.0
#3  0x00007f361f9bb90d in clone () from /lib64/libc.so.6
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[golang, cmd会泄露文件句柄]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/03/25/go-leak-fd/"/>
    <updated>2014-03-25T22:34:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/03/25/go-leak-fd</id>
    <content type="html"><![CDATA[<p>在go中用<code>cmd</code>生成新的process时, 在某些os中(包括linux的某些版本), 父进程的文件句柄会泄露到子进程中, 参看代码</p>

<pre><code>package main

import (
    "fmt"
    "os"
    "os/exec"
)

func main() {
    a, _ := os.OpenFile("1", os.O_CREATE|os.O_RDWR, 0755)
    defer a.Close()
    cmd := exec.Command("sh", "-c", "lsof +D .; sleep 3")
    output, _ := cmd.CombinedOutput()
    fmt.Printf("%v\n", string(output))
}
</code></pre>

<p>得到输出</p>

<pre><code>[root@GroupH-HA-1 tmp]# uname -a
Linux GroupH-HA-1 2.6.18-194.el5xen #1 SMP Tue Mar 16 22:01:26 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux
[root@GroupH-HA-1 tmp]# ./main
COMMAND  PID USER   FD   TYPE DEVICE    SIZE    NODE NAME
bash    4693 root  cwd    DIR  253,0   32768 3506177 .
main    6184 root  cwd    DIR  253,0   32768 3506177 .
main    6184 root  txt    REG  253,0 2250464 3506237 ./main
main    6184 root    3u   REG  253,0       0 3506238 ./1
sh      6189 root  cwd    DIR  253,0   32768 3506177 .
sh      6189 root    3u   REG  253,0       0 3506238 ./1
lsof    6190 root  cwd    DIR  253,0   32768 3506177 .
lsof    6191 root  cwd    DIR  253,0   32768 3506177 .
</code></pre>

<p>可以看到<code>./1</code>的文件句柄泄漏到了<code>sh -c</code>中, 目前为止没有特别好的解决方案</p>

<p>参看<a href="https://code.google.com/p/go/issues/detail?id=2603">此处bug描述</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[golang, windows和linux上的文件锁]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/03/20/go-file-lock/"/>
    <updated>2014-03-20T22:43:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/03/20/go-file-lock</id>
    <content type="html"><![CDATA[<p>直接上代码, <code>LockFile</code>可以获得一个文件的独占权, 或阻塞等待</p>

<h4>linux</h4>

<pre><code>func LockFile(file *os.File) error {
    return syscall.Flock(int(file.Fd()), syscall.LOCK_EX)
}
</code></pre>

<h4>windows</h4>

<pre><code>func LockFile(file *os.File) error {
    h, err := syscall.LoadLibrary("kernel32.dll")
    if err != nil {
        return err
    }
    defer syscall.FreeLibrary(h)

    addr, err := syscall.GetProcAddress(h, "LockFile")
    if err != nil {
        return err
    }
    for {
        r0, _, _ := syscall.Syscall6(addr, 5, file.Fd(), 0, 0, 0, 1, 0)
        if 0 != int(r0) {
            break
        }
        time.Sleep(100 * time.Millisecond)
    }
    return nil
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[推荐下我修改的gen]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/03/02/gen/"/>
    <updated>2014-03-02T21:29:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/03/02/gen</id>
    <content type="html"><![CDATA[<p><a href="http://clipperhouse.github.io/gen/"><code>gen</code></a>是go的代码生成器, 提供类似于<code>underscore</code>的函数集.</p>

<p>尝试将<code>gen</code>用在项目上,发现不太方便,对源码做了如下两个修改, 修改后的代码在<a href="https://github.com/ikarishinjieva/gen">这里</a>:</p>

<h4>1. 支持条件编译</h4>

<p>go提供了条件编译,根据<code>GOOS</code>和<code>GOARCH</code>进行交叉编译,也可以利用<a href="http://golang.org/cmd/go"><code>build tags</code></a>自定义条件编译</p>

<p>修改前可能碰到的问题是存在<code>a_linux.go</code>和<code>a_windows.go</code>, 分别定义一个函数<code>A</code>的两个版本. 调用<code>gen</code>时会报错:<code>A</code>不可以重复定义</p>

<p>这个修改已经被merge回原分支</p>

<h4>2. 对于import的其它包, 支持分析其源码</h4>

<p>设想一个场景, 存在<code>root/A</code>和<code>root/B</code>两个包, <code>root/B</code> import <code>root/A</code></p>

<p>在<code>root/B</code>上调用<code>gen</code>, <code>gen</code>会分析import关系, 找到并分析<code>root/A</code></p>

<p>在修改之前, 由于<code>gen</code>只使用了<code>types.Check</code>, 默认只会使用<code>gcimport</code>,只分析<code>root/A</code>编译好的pkg(<code>.a</code>文件), 而不包括<code>root/A</code>的源码.</p>

<p>也就是说对于所有依赖, 必须都保证其跑过<code>go install</code>, 才能在下游模块使用<code>gen</code>. 这个并不方便</p>

<p>做的修改是使用<code>go.tools/importer</code>代替<code>gcimporter</code>, 既可以分析编译好的pkg, 又可以分析源码</p>

<p>不过这个修改的代价是分析的时间会比较长</p>

<p>这个修改尚未被原分支接受</p>

<h4>3. <code>types</code>源码分析的一个问题</h4>

<p>以下代码在分析源码时报错, 但编译时是通过的</p>

<pre><code>c := make(chan os.Signal, 1)
signal.Notify(c, syscall.SIGTTIN)
</code></pre>

<p>分析时报的错是</p>

<pre><code>cannot pass argument c (variable of type chan os.Signal) to parameter of type chan&lt;- os.Signal
</code></pre>

<p>目前无解, 但结论是用<code>types</code>包进行的源码分析结果和编译时的略有差异</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GO exec.command.Wait 执行后台程序,在重定向输出时卡住]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/02/22/go-exec-command-block-when-redirect-stdout/"/>
    <updated>2014-02-22T10:30:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/02/22/go-exec-command-block-when-redirect-stdout</id>
    <content type="html"><![CDATA[<p>在GO上发现以下现象</p>

<pre><code>c := exec.Command("sh", "-c", "sleep 100 &amp;")
var b bytes.Buffer
c.Stdout = &amp;b

if e := c.Start(); nil != e {
    fmt.Printf("ERROR: %v\n", e)
}
if e := c.Wait(); nil != e {
    fmt.Printf("ERROR: %v\n", e)
}
</code></pre>

<p>这个代码会一直等到<code>sleep 100</code>完成后才退出, 与常识不符.</p>

<p>但去掉Stdout重定向后, 代码就不会等待卡住</p>

<pre><code>c := exec.Command("sh", "-c", "sleep 100 &amp;")
if e := c.Start(); nil != e {
    fmt.Printf("ERROR: %v\n", e)
}
if e := c.Wait(); nil != e {
    fmt.Printf("ERROR: %v\n", e)
}
</code></pre>

<p>在运行时打出stacktrace, 再翻翻GO的源代码, 发现GO卡在以下代码</p>

<pre><code>func (c *Cmd) Wait() error {
    ...
    state, err := c.Process.Wait()
    ...
    var copyError error
    for _ = range c.goroutine {
        if err := &lt;-c.errch; err != nil &amp;&amp; copyError == nil {
            copyError = err
        }
    }
    ...
}
</code></pre>

<p>可以看到<code>Wait()</code>在等待Process结束后, 还等待了所有<code>c.goroutine</code>的<code>c.errch</code>信号. 参看以下代码:</p>

<pre><code>func (c *Cmd) stdout() (f *os.File, err error) {
    return c.writerDescriptor(c.Stdout)
}

func (c *Cmd) writerDescriptor(w io.Writer) (f *os.File, err error) {
    ...
    c.goroutine = append(c.goroutine, func() error {
        _, err := io.Copy(w, pr)
        return err
    })
    ...
}
</code></pre>

<p>重定向<code>stdout</code>时, 会添加一个监听任务到<code>goroutine</code> (<code>stderr</code>也是同理)</p>

<p>结论是由于将<code>sleep 100</code>放到后台执行, 其进程<code>stdout</code>并没有关闭, <code>io.Copy()</code>不会返回, 所以会卡住</p>

<p>临时的解决方法就是将后台进程的<code>stdout</code>和<code>stderr</code>重定向出去, 以下代码不会卡住:</p>

<pre><code>c := exec.Command("sh", "-c", "sleep 100 &gt;/dev/null 2&gt;/dev/null &amp;")
var b bytes.Buffer
c.Stdout = &amp;b

if e := c.Start(); nil != e {
    fmt.Printf("ERROR: %v\n", e)
}
if e := c.Wait(); nil != e {
    fmt.Printf("ERROR: %v\n", e)
}
</code></pre>

<p>已经报了<a href="https://code.google.com/p/go/issues/detail?id=7378&amp;thanks=7378&amp;ts=1392967848">bug</a></p>

<p>但想不出好的GO代码的修改方案</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[尝试使用mysql plugin将RESET SLAVE后的节点重新恢复成slave]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/02/11/try-rollback-master-back-to-slave-by-mysql-plugin/"/>
    <updated>2014-02-11T22:31:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/02/11/try-rollback-master-back-to-slave-by-mysql-plugin</id>
    <content type="html"><![CDATA[<p>这几天在尝试为以下场景制作一个mysql plugin, 但是是一个失败的尝试, 在此记录</p>

<pre><code>一对mysql主从节点 M-S, 节点S执行了RESET SLAVE
后来后悔了
在没有数据通过非replication的渠道写入S的条件下, 想让S和M重新恢复成一对主从
</code></pre>

<p>关键点是S能将<code>RESET SLAVE</code>时S的<code>Exec_Master_Log_Pos</code>和<code>S binlog pos</code>记录下来</p>

<p>尝试了以下几种方案:</p>

<h5>1. 调用者在<code>RESET SLAVE</code>时手工记录, 不需要制作插件</h5>

<hr />

<h5>2. Audit plugin.</h5>

<p>Mysql的Audit plugin可以审计大部分mysqld经手的SQL, 包括<code>RESET SLAVE</code>.</p>

<p>但Audit plugin是在每个SQL之后才会调用. 在<code>RESET SLAVE</code>时S上master_info会被清理, 即<code>Exec_Master_Log_Pos</code>的信息在调用Audit plugin已经丢失</p>

<hr />

<h5>3. Replication plugin (<code>after_reset_slave</code>)</h5>

<p>Replication plugin (参看mysql semisync的源码), 在slave端提供了<code>Binlog_relay_IO_observer</code>, 贴个Mysql源码方便理解</p>

<pre><code>/**
    Observes and extends the service of slave IO thread.
 */
 typedef struct Binlog_relay_IO_observer {
   uint32 len;

   /**
      This callback is called when slave IO thread starts

      @param param Observer common parameter

      @retval 0 Sucess
      @retval 1 Failure
   */
   int (*thread_start)(Binlog_relay_IO_param *param);

   /**
      This callback is called when slave IO thread stops

      @param param Observer common parameter

      @retval 0 Sucess
      @retval 1 Failure
   */
   int (*thread_stop)(Binlog_relay_IO_param *param);

   /**
      This callback is called before slave requesting binlog transmission from master

      This is called before slave issuing BINLOG_DUMP command to master
      to request binlog.

      @param param Observer common parameter
      @param flags binlog dump flags

      @retval 0 Sucess
      @retval 1 Failure
   */
   int (*before_request_transmit)(Binlog_relay_IO_param *param, uint32 flags);

   /**
      This callback is called after read an event packet from master

      @param param Observer common parameter
      @param packet The event packet read from master
      @param len Length of the event packet read from master
      @param event_buf The event packet return after process
      @param event_len The length of event packet return after process

      @retval 0 Sucess
      @retval 1 Failure
   */
   int (*after_read_event)(Binlog_relay_IO_param *param,
                           const char *packet, unsigned long len,
                           const char **event_buf, unsigned long *event_len);

   /**
      This callback is called after written an event packet to relay log

      @param param Observer common parameter
      @param event_buf Event packet written to relay log
      @param event_len Length of the event packet written to relay log
      @param flags flags for relay log

      @retval 0 Sucess
      @retval 1 Failure
   */
   int (*after_queue_event)(Binlog_relay_IO_param *param,
                            const char *event_buf, unsigned long event_len,
                            uint32 flags);

   /**
      This callback is called after reset slave relay log IO status

      @param param Observer common parameter

      @retval 0 Sucess
      @retval 1 Failure
   */
   int (*after_reset_slave)(Binlog_relay_IO_param *param);
 } Binlog_relay_IO_observer;
</code></pre>

<p>首先尝试用<code>after_reset_slave</code>, 从函数名字就可以看到会遇到和Audit Plugin相同的问题: 即<code>Exec_Master_Log_Pos</code>的信息在调用时已经丢失</p>

<hr />

<h5>4. Replication plugin (<code>after_reset_slave</code>再尝试, <code>future_group_master_log_pos</code>)</h5>

<p>还不死心, <code>Exec_Master_Log_Pos</code>的数据结构是<code>Relay_log_info.group_master_log_pos</code>, 尽管这个信息在<code>after_reset_slave</code>时已经丢失, 但发现<code>Relay_log_info.future_group_master_log_pos</code>可能是个方向</p>

<p>先解释<code>Relay_log_info.future_group_master_log_pos</code>, 可以参看<code>log_event.cc</code>的这段注释</p>

<pre><code>  /*
    InnoDB internally stores the master log position it has executed so far,
    i.e. the position just after the COMMIT event.
    When InnoDB will want to store, the positions in rli won't have
    been updated yet, so group_master_log_* will point to old BEGIN
    and event_master_log* will point to the beginning of current COMMIT.
    But log_pos of the COMMIT Query event is what we want, i.e. the pos of the
    END of the current log event (COMMIT). We save it in rli so that InnoDB can
    access it.
  */
  const_cast&lt;Relay_log_info*&gt;(rli)-&gt;future_group_master_log_pos= log_pos;
</code></pre>

<p><code>future_group_master_log_pos</code>指向了execute的最后一个transaction的COMMIT event之前, 即<code>future_group_master_log_pos</code> 大部分时间等于 <code>group_master_log_pos - 27</code> (27是COMMIT event的长度)</p>

<p>但仍有例外情况: 如果M执行了<code>FLUSH LOGS</code>, 将log从0001递增到了0002, 此时S上的<code>future_group_master_log_pos</code>会指向0001的最后一个transaction的COMMIT event之前. 但S上的<code>group_master_log_name</code>已经到了0002, 与<code>future_group_master_log_pos</code>不匹配, 会引起异常</p>

<p>(其实此时S上的<code>group_master_log_name</code>也已经置空了, 但可以从内存残片中恢复出文件名)</p>

<p>设想如果对于log_name也有<code>future_group_master_log_name</code>, 那么S可以直接<code>change master</code>到M的<code>future_group_master_log_name</code>和<code>future_group_master_log_pos</code>位置, 可以恢复起M-S主从结构</p>

<hr />

<h5>5. Replication plugin (<code>thread_stop</code>)</h5>

<p>Replication plugin的<code>thread_stop</code>是指Slave IO thread停止时调用, 此时可以拿到<code>Exec_Master_Log_Pos</code>和<code>S binlog pos</code>, 但拿到的<code>S binlog pos</code>没有意义, 因为不能保证Slave SQL thread也停下来了</p>

<hr />

<h5>6. Storage Engine plugin</h5>

<p>这是我最后一根救命稻草, 阅读Mysql源码时注意到以下片段(做了缩减)</p>

<pre><code>int reset_slave(THD *thd, Master_info* mi)
{
    ...
    ha_reset_slave(thd);
    ... //clean memory data
}
</code></pre>

<p><code>reset_slave</code>在清理内存数据前通知了storage engine插件, 这个插件可以获得所有必要信息</p>

<p>但存在一个问题, 即<code>ha_reset_slave</code>仅在Mysql NDB版本中存在, 不具备通用性, 参看宏定义(做了缩减)</p>

<pre><code>#ifdef HAVE_NDB_BINLOG
...
void ha_reset_slave(THD *thd);
...
#else
...
#define ha_reset_slave(a) do {} while (0)
...
#endif
</code></pre>

<hr />

<h4>吐槽和总结</h4>

<p>可以看到Mysql plugin不<strong>太</strong>预留接口, 是仅仅为已知应用场景提供必要接口, 比如<code>Binlog_relay_IO_observer</code>中有<code>after</code>不一定有<code>before</code>. 比较容易控制插件质量, 但插件能做到的非常局限.</p>

<p>以上各种尝试, 归根到底, 只要修改Mysql的一点源码编译一下就可以达到很好的效果, 不需要用插件的方式在Mysql中到处找功能插槽, 但通用性变差.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译mysql插件的碰到的问题]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/01/28/compile-mysql-plugin/"/>
    <updated>2014-01-28T16:48:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/01/28/compile-mysql-plugin</id>
    <content type="html"><![CDATA[<p>最近尝试制作了<a href="https://github.com/ikarishinjieva/mysql_plugin-binlog_dump_list">一个mysql的插件</a>. 对c/c++的编译不熟, 又是第一次尝试做mysql插件, 编译过程中碰到些状况</p>

<p>编写好mysql插件后, 编译成功, 在mysql中安装运行报错: 取了<code>threads</code>中的THD, 其中THD->thread_id值为空</p>

<p>由于是mysql内置的数据结构, 一时没了头绪, 只能通过gdb连上去看看</p>

<p>发现plugin打印出来的thread_id距离THD开头的距离为</p>

<pre><code>tmp=0x3661f80
&amp;tmp-&gt;thread_id=0x36637b0
delta = 0x1830
</code></pre>

<p>而gdb打印出来的距离为</p>

<pre><code>(gdb) p tmp
$1 = (THD *) 0x3661f80
(gdb) p &amp;tmp-&gt;thread_id
$2 = (my_thread_id *) 0x3663878
delta = 0x18F8
</code></pre>

<p>结论很显然, plugin编译的THD结构和mysqld的THD结构不匹配, 即plugin的编译参数和mysqld的编译参数不一样.</p>

<p>当然mysql的文档上只会说一句大意是 &#8221;<strong>编译参数应当设置成一样的</strong>&#8220;的话</p>

<p>其中比较重要的几个编译选项</p>

<ol>
<li>DBUG_ON</li>
<li>SAFE_MUTEX</li>
<li>DBUG_OFF (不设置DBUG_ON并不等于DBUG_OFF)</li>
</ol>


<p>这几个选项会影响当使用mysqld内部数据结构的长度, 不排除还有其他</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[jruby中tcp阻塞时Timeout::timeout失效]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2014/01/08/jruby-bug-tcp-timeout/"/>
    <updated>2014-01-08T23:04:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2014/01/08/jruby-bug-tcp-timeout</id>
    <content type="html"><![CDATA[<h3>问题场景</h3>

<p>首先有一台tcp server, 模拟一个黑洞</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s1">&#39;socket&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="n">tcp_server</span> <span class="o">=</span> <span class="no">TCPServer</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="s2">&quot;0.0.0.0&quot;</span><span class="p">,</span> <span class="mi">6666</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="kp">loop</span> <span class="k">do</span>
</span><span class='line'>     <span class="n">socket</span> <span class="o">=</span> <span class="n">tcp_server</span><span class="o">.</span><span class="n">accept</span>
</span><span class='line'>     <span class="nb">puts</span> <span class="s1">&#39;got conn&#39;</span><span class="o">]</span>
</span><span class='line'>     <span class="c1">#blackhole</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>然后发起一个connection, 从server接受消息(很显然会阻塞在recv上), 并用<code>Timeout::timeout</code>设置一个超时时间</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s2">&quot;socket&quot;</span>
</span><span class='line'><span class="nb">require</span> <span class="s2">&quot;timeout&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">sock</span> <span class="o">=</span> <span class="no">Socket</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="ss">Socket</span><span class="p">:</span><span class="ss">:AF_INET</span><span class="p">,</span> <span class="ss">Socket</span><span class="p">:</span><span class="ss">:SOCK_STREAM</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span class='line'><span class="n">addr</span> <span class="o">=</span> <span class="no">Socket</span><span class="o">.</span><span class="n">sockaddr_in</span><span class="p">(</span><span class="mi">6666</span><span class="p">,</span> <span class="s2">&quot;127.0.0.1&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">sock</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">addr</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="ss">Timeout</span><span class="p">:</span><span class="ss">:timeout</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>     <span class="n">sock</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>上面这个场景如果在ruby上跑,5秒后会超时,但如果使用jruby(1.7.6)就会一直处于阻塞</p>

<h3>解决方案</h3>

<p>使用非阻塞<code>recv</code>,可以在jruby上正常运行</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s2">&quot;socket&quot;</span>
</span><span class='line'><span class="nb">require</span> <span class="s2">&quot;timeout&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">sock</span> <span class="o">=</span> <span class="no">Socket</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="ss">Socket</span><span class="p">:</span><span class="ss">:AF_INET</span><span class="p">,</span> <span class="ss">Socket</span><span class="p">:</span><span class="ss">:SOCK_STREAM</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span class='line'><span class="n">addr</span> <span class="o">=</span> <span class="no">Socket</span><span class="o">.</span><span class="n">sockaddr_in</span><span class="p">(</span><span class="mi">6666</span><span class="p">,</span> <span class="s2">&quot;127.0.0.1&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">sock</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">addr</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="ss">Timeout</span><span class="p">:</span><span class="ss">:timeout</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="k">begin</span>
</span><span class='line'>        <span class="n">sock</span><span class="o">.</span><span class="n">recv_nonblock</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'>    <span class="k">rescue</span> <span class="ss">IO</span><span class="p">:</span><span class="ss">:WaitReadable</span>
</span><span class='line'>        <span class="no">IO</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="o">[</span><span class="n">sock</span><span class="o">]</span><span class="p">,</span><span class="kp">nil</span><span class="p">,</span><span class="kp">nil</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</span><span class='line'>        <span class="k">retry</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>猜测</h3>

<p>查看一下ruby <code>timeout.rb</code>的源码</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'>  <span class="k">begin</span>
</span><span class='line'>    <span class="n">x</span> <span class="o">=</span> <span class="no">Thread</span><span class="o">.</span><span class="n">current</span>
</span><span class='line'>    <span class="n">y</span> <span class="o">=</span> <span class="no">Thread</span><span class="o">.</span><span class="n">start</span> <span class="p">{</span>
</span><span class='line'>      <span class="k">begin</span>
</span><span class='line'>        <span class="nb">sleep</span> <span class="n">sec</span>
</span><span class='line'>      <span class="k">rescue</span> <span class="o">=&gt;</span> <span class="n">e</span>
</span><span class='line'>        <span class="n">x</span><span class="o">.</span><span class="n">raise</span> <span class="n">e</span>
</span><span class='line'>      <span class="k">else</span>
</span><span class='line'>        <span class="n">x</span><span class="o">.</span><span class="n">raise</span> <span class="n">exception</span><span class="p">,</span> <span class="s2">&quot;execution expired&quot;</span>
</span><span class='line'>      <span class="k">end</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="k">return</span> <span class="k">yield</span><span class="p">(</span><span class="n">sec</span><span class="p">)</span>
</span><span class='line'>  <span class="k">ensure</span>
</span><span class='line'>    <span class="k">if</span> <span class="n">y</span>
</span><span class='line'>      <span class="n">y</span><span class="o">.</span><span class="n">kill</span>
</span><span class='line'>      <span class="n">y</span><span class="o">.</span><span class="n">join</span> <span class="c1"># make sure y is dead.</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>大概看到timeout是起了一个计时线程,超时时向主线程发起exception</p>

<p>猜测是因为jvm的线程模型导致exception不能向阻塞线程提交,但有待验证</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[栽在Go中for的变量]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/12/26/go-iterate-variable/"/>
    <updated>2013-12-26T22:13:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/12/26/go-iterate-variable</id>
    <content type="html"><![CDATA[<p>我是万没料到自己栽在了go的for上，说多了都是眼泪</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='go'><span class='line'><span class="kd">type</span> <span class="nx">testStruct</span> <span class="kd">struct</span> <span class="p">{</span>
</span><span class='line'>     <span class="nx">no</span> <span class="kt">int</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>     <span class="nx">a</span> <span class="o">:=</span> <span class="p">[]</span><span class="nx">testStruct</span><span class="p">{</span><span class="nx">testStruct</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span> <span class="nx">testStruct</span><span class="p">{</span><span class="mi">2</span><span class="p">},</span> <span class="nx">testStruct</span><span class="p">{</span><span class="mi">3</span><span class="p">}}</span>
</span><span class='line'>     <span class="kd">var</span> <span class="nx">p</span> <span class="o">*</span><span class="nx">testStruct</span>
</span><span class='line'>     <span class="k">for</span> <span class="nx">_</span><span class="p">,</span> <span class="nx">i</span> <span class="o">:=</span> <span class="k">range</span> <span class="nx">a</span> <span class="p">{</span>
</span><span class='line'>          <span class="k">if</span> <span class="nx">i</span><span class="p">.</span><span class="nx">no</span> <span class="o">==</span> <span class="mi">2</span> <span class="p">{</span>
</span><span class='line'>               <span class="c1">// o := i</span>
</span><span class='line'>               <span class="c1">// p = &amp;o</span>
</span><span class='line'>               <span class="nx">p</span> <span class="p">=</span> <span class="o">&amp;</span><span class="nx">i</span>
</span><span class='line'>          <span class="p">}</span>
</span><span class='line'>     <span class="p">}</span>
</span><span class='line'>     <span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">p</span><span class="p">.</span><span class="nx">no</span><span class="p">)</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>猜猜看输出是多少？<a href="http://play.golang.org/p/OzkxuYIboc">试试看吧</a></p>

<p>理解起来很容易，<code>p</code>取得是<code>i</code>的地址，而<strong>range循环变量<code>i</code>在每个循环之间都是复用同一个地址</strong></p>

<p>证明一下，<a href="http://play.golang.org/p/b3QFcoh35Q">试试看？</a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='go'><span class='line'><span class="nx">a</span> <span class="o">:=</span> <span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">}</span>
</span><span class='line'><span class="k">for</span> <span class="nx">_</span><span class="p">,</span> <span class="nx">item</span> <span class="o">:=</span> <span class="k">range</span> <span class="nx">a</span> <span class="p">{</span>
</span><span class='line'>     <span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&quot;%p\n&quot;</span><span class="p">,</span> <span class="o">&amp;</span><span class="nx">item</span><span class="p">)</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>虽然很容易理解，也很容易掉坑，尤其for上用<code>:=</code>，那感觉就像js里连续用<code>var</code>，除了第一下剩下的都不好使&#8230;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对于PaxosLease的个人理解 2]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/12/19/paxos-lease-2/"/>
    <updated>2013-12-19T20:19:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/12/19/paxos-lease-2</id>
    <content type="html"><![CDATA[<p>在阅读之前，请确定已浏览过<a href="http://ikarishinjieva.github.io/blog/blog/2013/12/19/paxos-lease/">第一篇</a>，并阅读过以下参考文献[1]（最好是阅读过参考文献[2]）</p>

<hr />

<h4>参考文献</h4>

<p>[1]<a href="http://dsdoc.net/paxoslease/index.html">【译】PaxosLease：实现租约的无盘Paxos算法</a></p>

<p>[2] <a href="https://github.com/scalien/keyspace/tree/master/src/Framework/PaxosLease">Keyspace源码</a></p>

<hr />

<p>本篇将讨论以下一些实现PaxosLease中的问题和解决：</p>

<ol>
<li>2PC 两阶段的超时设置</li>
<li>续租困境及解决</li>
<li>Proposer对HighestPromisedProposeId的学习</li>
<li>放弃租约</li>
</ol>


<hr />

<p>假设读者已经从参考文献[1]中熟悉了PaxosLease算法，在所有讨论之前，我们还是先简述一下整个PaxosLease算法，目的来统一一些术语。其中有一些空白，类似于<strong>{1}</strong>，后面的讨论中会将这些空白逐一填满：</p>

<p><strong>1</strong> Proposer A 想要获得租约，生成一个新的ProposeId，组装成一个Prepare Request，并广播给所有Accepter</p>

<p><strong>2</strong> Accepter B收到来自Proposer A的PrepareRequest，检查PrepareRequest中的ProposeId<br/>
若低于Accepter B的HighestPromisedProposeId，则忽略这一PrepareRequest，<strong>{5}</strong><br/>
否则</p>

<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将HighestPromisedProposeId置为PrepareRequest中的ProposeId


<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **{1}**


<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;且向Proposer A反馈Prepare Response，Response中带有Accepter B的AcceptedProposeId（可能为空）<br/>


<p><strong>3</strong> Proposer A收到Accepter B的Prepare Response。<br/>若多数派Accepter返回的PrepareRequest中的AcceptedProposeId都为空，<strong>{2}</strong>，则表示多数派Accepter都可以接受Proposer A的Propose，进入Propose 阶段：</p>

<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proposer A启动租约定时器β。定时器β超时时，重新启动Prepare阶段


<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proposer A向多数派广播ProposeRequest<br/>


<p><strong>4</strong> <strong>{6}</strong></p>

<p><strong>5</strong> Accepter B收到来自Proposer A的ProposeRequest，（再次）检查ProposeRequest中的ProposeId</p>

<br/>若低于Accepter B的HighestPromisedProposeId，则忽略这一ProposeRequest


<br/>否则：<br/>将HighestPromisedProposeId置为ProposeRequest中的ProposeId


<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;且将AcceptedProposeId置为ProposeRequest中的ProposeId


<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;且启动定时器γ。定时器γ超时时，将AcceptedProposeId置为0


<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;且向Proposer A反馈ProposeResponse<br/>


<p><strong>6</strong> Proposer A收到Accepter B的ProposeResponse。若Proposer A已收到多数派的ProposeReponse，则Proposer A:</p>

<br/>可以认为自己持有租约，租约到期的时间为定时器β的超时时间，租约到期时需要清理


<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**{4}**


<br/>**{3}**<br/>


<hr />

<h4>这一部分讨论2PC两阶段的超时设置</h4>

<p>PaxosLease的过程引自参考[1]中，可以看到Prepare阶段没有计时器，而在Propose阶段，Proposer和Accepter分别启动定时器β和定时器γ，来表示“Propose阶段在当前节点超时，需要重新启动投票”，或者“租约在当前节点过期，当前节点可以参与新一轮投票”</p>

<p>可以看到这个结论与<a href="http://ikarishinjieva.github.io/blog/blog/2013/12/19/paxos-lease/">上一篇</a>的描述不符，下面来解释原因</p>

<p>在这种设计下，若Prepare阶段有请求丢失，导致Proposer没法获得多数派的反馈，则Prepare阶段会僵死在那里</p>

<p>参考[1]中使用这种设计的原因是参考[1]中仅讨论了一次投票的情景，并不包括如何长期维护投票秩序。作为一个介绍性的文章，这种情景限定有助于读者理解</p>

<p>而实际应用中，我们必须要解决Prepare阶段的僵死情况，即在Prepare阶段也加入定时器（Keyspace中也是这样做的）：<br/>
<strong>{1}</strong> = &#8220;并启动定时器α，时长小于租约时长。α超时时，重新启动Prepare阶段。在节点进入Propose阶段时，取消α计时&#8221;</p>

<hr />

<h4>续租困境及解决</h4>

<p>在实际应用中，希望长时间维持一个节点master的身份，而不希望master在集群里换来换去，那么当前持有租约的节点就希望能成功续租</p>

<p>先介绍续租的实现，参考[1]中提到“如果多数派响应了空的提案或是 <em>已存在提案</em> （即这个提案中的该请求者的租约还没有过期），它可以再次提议自己为租约的持有者”，相应的我们在过程中做出修改：<br/>
<strong>{2}</strong> = &#8220;或PrepareRequest中的AcceptedProposeId=Proposer A的leaseProposeId&#8221;<br/>
<strong>{3}</strong> = &#8220;置leaseProposeId为当前的proposeId&#8221;<br/>
<strong>{4}</strong> = &#8220;置leaseProposeId为0&#8221;<br/></p>

<p>续租实现后，测试过程中，就会发现以下续租困境：</p>

<ol>
<li>Proposer A获得租约，proposeId=1（此处我们为了方便，简化ProposeId的结构为提交次数），等待一段时间t后开始续租</li>
<li>Proposer B在t的过程中，不断尝试想获取续约，但始终得不到多数派的批准，这个过程中Proposer B发出提案的proposeId会飙升，比如说proposeId=10</li>
<li>Accepter们在Proposer B的不断尝试中，Prepare阶段HighestPromisedProposeId也跟着飙升，比如说HighestPromisedProposeId=10</li>
<li>Proposer A开始续租，proposeId=2，续租失败</li>
</ol>


<p>解决续租困境有几种方式：</p>

<ol>
<li>Accepter在定时器γ超时前，不接受新的PrepareRequest</li>
<li>Proposer续租时，将Request的ProposeId增加较大的值</li>
</ol>


<p>我们随机选择第二种方式，那么在时间t（t&lt;租约时长）的过程中，Proposer B能发出的Prepare Request数量，即B的ProposeId增长量Δ一定满足：Δ &lt; ceil(租约时长/定时器α时长)。即，续租时，Proposer A的proposeId += ceil(租约时长/定时器α时长)即可。</p>

<p>额外一提，Keyspace中续租的时间（在获得租约后1s就开始续租）远小于Prepare/Propose阶段的超时时间，不会触发这个困境</p>

<hr />

<h4>Proposer对HighestPromisedProposeId的学习</h4>

<p>解决了续租困境后，我们再设定一种困境：</p>

<ol>
<li>Proposer A 持有租约，并一直续租，导致ProposeId变得很大，比如ProposeId = 10000</li>
<li>Accepter们的HighestPromisedProposeId也变得很大，比如HighestPromisedProposeId = 10000</li>
<li>此时Proposer A离线</li>
<li>Proposer B 闪亮登场（比如在集群中补充了一台server），欲接手，发出PrepareRequest，ProposeId = 0</li>
<li>Proposer B 想要持有租约，至少要经过近10000次重试，才能将ProposeId增大到Accepter可接受的大小</li>
</ol>


<p>这个困境的关键是在Proposer知晓的ProposeId太过落后于集群内最新的ProposeId，导致Proposer无法短时间内获得租约，而是需要长时间的重试</p>

<p>有两种解决方案可供参考：</p>

<ol>
<li>Keyspace用的是这种方法：认为一个节点由Proposer和Accepter构成，即同一个节点Proposer和Accepter可以共享HighestPromisedProposeId。<br/>这样在上述情况下，只要集群里有任何一个Request被发给Accepter B，那么Proposer B 也可以学习到集群最新的ProposeId</li>
<li>另一种方案是加入PrepareReject消息，在消息中Accpeter加入HighestPromisedProposeId供Promised学习，即<br/>
<strong>{5}</strong> = 向Proposer A反馈Prepare Reject，其中带有HighestPromisedProposeId<br/>
<strong>{6}</strong> = 若Proposer A收到Accepter B的Prepare Reject，则学习其中的HighestPromisedProposeId</li>
</ol>


<p>第二种方案还需考虑网络延迟的情况，即Prepare Reject被长期延迟的情况。不过这种延迟并不会带来影响，因为</p>

<ol>
<li>Proposer较晚学习到HighestPromisedProposeId，不会发生错误，只会延迟产生正确的投票结果</li>
<li>Prepare Reject的处理过程中，Proposer也会判断此消息是否是当前Request产生的Response。若Prepare Reject延迟较长，Proposer会发起新一轮的Prepare Request，收到旧的Prepare Reject则会抛弃此消息。</li>
</ol>


<hr />

<h4>放弃租约</h4>

<p>实际应用中，会碰到这种情况：持有租约的服务器同时肩负着其他资源的“敏感角色”，比如数据库集群中的主服务器。此时若此server挂掉，则需要等待租约过期，重新投票产生新的租约持有者，然后再由新的租约持有者裁决出新的数据库主服务器，并进行数据保护迁移。这个过程由于集群租约持有者和数据库主服务器这两种角色同时消失，会带来较大的影响。</p>

<p>为解决以上情况，就希望若两种角色重叠在一台server上，此时server能在<strong>运行稳定时</strong>放弃租约，做法有两种：</p>

<ol>
<li><p>如参考[1]中提到：“请求者可以发送一个特定释放消息给接受者，消息中包含了它要释放租给的投票编号。在发送释放消息之前，请求者把内部状态从“我持有租约”切换到“我没有持有租约”。”。<br/><br/>
考虑“释放消息”被长期延迟的情况，最坏的情况是没有Accepter及时收到消息，跟正常租约超时一样，需要等到多数派Accepter的定时器γ都超时，才能选出新的master。看来不会对正确性带来较大影响，只是不能及时释放租约。</p></li>
<li><p>懒惰一点，就等到租约超时。为了不让当前Proposer再成为租约持有者，将当前Proposer“冻结” 2倍租约时长，这样既可以等到当前租约超时，又可以避免参与到下一轮投票。<br/><br/>
所谓“冻结”，即停掉一切定时器，且不发出新的request</p></li>
</ol>


<hr />

<p>至此，此篇已经描述了在实现PaxosLease中碰到的一切问题和取舍，欢迎各位反馈。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对于PaxosLease的个人理解 1]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/12/19/paxos-lease/"/>
    <updated>2013-12-19T20:19:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/12/19/paxos-lease</id>
    <content type="html"><![CDATA[<p>Paxos是分布式解决数据一致性的算法，而PaxosLease是Paxos中的一个子集，用于在集群中选择出一个节点作为master</p>

<p>最近因为项目需要，实现了一下PaxosLease，代码放在<a href="https://github.com/ikarishinjieva/PaxosLease-go">Github</a>上</p>

<p>计划用两篇blog，分别记录下自己对PaxosLease的理解，以及对PaxosLease应用时的一些变化。<br/>这篇是我对PaxosLease的个人理解，如有任何bug/改进的建议，欢迎comment</p>

<hr />

<p>首先，定义一下问题场景：有N个节点组成一个集群，需要从其中选出一个master。（其中可能遇到节点间传输的信息延迟/丢失，节点离线，网络脑裂等等状况）</p>

<p>在这个定义中，先要解决如何定义“选出一个master”，有以下几种可能</p>

<ol>
<li>仅有master节点知道自己是master，其他节点只知道自己不是master，而不知道谁是master</li>
<li>所有在线节点都知道谁是master，离线节点上线后要等待下次选举</li>
<li>所有节点都知道谁是master，离线节点上线后要立刻学习谁是master</li>
</ol>


<p>PaxosLease选择的是第一种方式（在此只讨论没有Learner的情况，有Learner的情况可以实现另外两种情况），这种方式满足现在的项目需要</p>

<p>这一个部分的结论：在以上的定义下，PaxosLease需要满足一个<em>不定式</em>：在同一时间，集群内至多有一个节点认为自己是master</p>

<hr />

<p>要在多个节点达成一致，最通常的想法是二段提交（2-phase commit，2PC），经典2PC步骤是：</p>

<ol>
<li>某节点A向其他所有节点发起PrepareRequest（准备请求）</li>
<li>某节点B收到节点A的PrepareRequest，经过状态检查，将自己的状态置为PrepareReady（表示可以接受A的PrepareRequest）<br/>并向A发送PrepareResponse（准备请求的回复）</li>
<li>A收到其他节点发来的PrepareResponse，当满足<em>某个条件</em>时，A认为集群整体同意了他得PrepareRequest。<br/>于是A向其他所有节点发出CommitRequest（提交请求）</li>
<li>节点B收到A的CommitRequest，且检查到<em>当前状态是为A准备的状态</em>，则向A发送CommitResponse（提交回复）</li>
<li>A收到其他节点发来的CommitResponse，当满足<em>某个条件</em>时，A认为集群已经完成了提交</li>
</ol>


<p><img src="http://ikarishinjieva.github.com/blog/images/2013-12-19-paxos-lease-1.png"></p>

<p>但经典的2PC没有以下解决的问题：</p>

<ol>
<li>如何处理网络脑裂</li>
<li>如何解决master突然离线，比如网络故障</li>
<li>如何面对选举过程中通信可能发生的延迟和中断</li>
<li>是否会发生动态死锁</li>
</ol>


<hr />

<p>这一部分先解决网络脑裂的问题。比如有5个节点的集群，分割为[1,2]和[3,4,5]，那么在[1,2]子集群中不能选举出master，而在[3,4,5]子集群中必须要选举出master。若脑裂前master落在[1,2]，那么租约到期后，[1,2]不能选举出master</p>

<p>解决方案是：指定2PC步骤中的<em>某个条件</em>为“收到集群中超过集群节点数一半的节点（多数派）的正反馈”。那么[3,4,5]子集群可能选出master，而[1,2]由于只有2个节点，小于ceil(5/2) = 3，没法选出master</p>

<p>在此，我们称一个集群中，超过集群节点数一半的节点集合为多数派</p>

<hr />

<p>这一部分将解决master突然离线的问题。根据<em>不变式</em>，除了master本身，没有节点知道谁是master。在这种情况下，如果不做点什么，集群就不会再有master了</p>

<p>解决方案是使用租约，即谁持有租约谁是master。</p>

<br/>由于是多数派选举master时，选举出master时，多数派的每个节点都会开始一个定时器，时长和租约时长相同


<br/>于是在租约过期前，多数派的定时器都不会超时，多数派不会参与投票，即集群选不出新的master


<br/>那么在租约过期后，多数派的定时器都超时，可以投票，集群就可以重新选举出master


<br/>整个过程与离线的master没有任何交互，也就可以在master离线时选举出新的master


<p>且整个过程中，租约到期前（多数派定时器超时前），集群不会有两个master同时存在，即满足<em>不定式</em></p>

<p>以上<em>等到租约过期</em>的做法有一个前提，即所有节点都知道统一的租约时长 （此处是时长，而不是过期时间。PaxosLease并不要求各个节点时钟同步，因此必须使用时长）。这是时长往往是静态配置，而不是动态协商的</p>

<hr />

<p>这一部分将解决选举过程中通信可能发生延迟。</p>

<p>发生延迟意味着A-B已经进入了下一轮投票，C可能才完成上一轮投票，C的反馈可能影响到这一轮投票结果。</p>

<p>此处PaxosLease引入了投票ID（PaxosLease称ProposeId，后面会统一名称的）的概念，投票ID对于某一节点A，在全局是单调递增的。常用的投票ID结构为&lt;投票轮数 | 重启计数 | 节点ID>（“|”为字符串拼接），这个ID可以被持久化存储，即新一轮投票或节点重启时投票ID都会单调递增。</p>

<p>有了投票ID，那么节点可以只响应本轮的反馈，而不受其他轮的干扰。</p>

<hr />

<p>这一部分将解决选举过程中通信可能发生中断。</p>

<p>发生中断意味着2PC某阶段会一直等待多数派的反馈，但反馈都丢失了，于是选举可能被无限期拖延下去。很容易得出解决方案：设置超时时间。即在2PC每一个阶段都设置超时时间，若超时，则回退重新开始新一轮的2PC</p>

<hr />

<p>这一部分将讨论如何解决动态死锁</p>

<p>首先说明何为动态死锁，比如有4个节点的集群[1,2,3,4]，1和4同时请求自己为master，1的request发给2，而4的request发给3，没有任何一方获得多数派，于是进入新的一轮，以上状况重复出现，陷入死循环，没法选出master</p>

<p>观察这个问题的症结在于，2收到1的prepare request后，进入PrepareReady状态，将不再接受4发来的请求。这样[1,2]和[3,4]不断对撞，陷入死锁。</p>

<p>解决方案是PaxosLease引入了“不稳定”的PrepareReady，即2进入为1准备的PrepareReady状态后，如果收到4的PrepareRequest，且这个投票ID大于来自1的PrepareRequest的投票ID，则2转而进入为4准备的PrepareReady</p>

<p>可以看到投票ID代表了优先级，也就能理解之前要求<strong>某节点</strong>的投票ID单调递增的理由了</p>

<p>需要说明的是，上述做法只是大大降低动态死锁的概率，但仍然可能存在小概率的动态死锁，即两个节点1和4不断增大投票ID且在2和3进入Commit之前不断抢占2和3，形成竞争，可以引入随机的等待来规避这个小概率事件</p>

<hr />

<p>在此，将上面描述的名词对应到PaxosLease算法的术语上</p>

<ul>
<li>租约 = lease（租约）</li>
<li>投票 = propose (提案)</li>
<li>发出request的节点 = proposer</li>
<li>接受request，发出response的节点 = accepter</li>
<li>CommitRequest = ProposeRequest</li>
<li>CommitResponse = ProposeReponse</li>
<li>投票ID = ProposeID</li>
</ul>


<p>之后将使用术语</p>

<hr />

<p>以上，是出于个人理解，来理解PaxosLease的几个重要元素：</p>

<ol>
<li>lease</li>
<li>propose</li>
<li>ProposeId</li>
<li>两阶段的超时设置</li>
<li>多数派形成决议</li>
</ol>


<p>一些实现上的问题会在下一篇blog讨论</p>

<hr />

<p>再次讨论<em>不变式</em></p>

<p>在某一时刻，集群中最多存在一个Proposer，知道自己获得了租约。</p>

<p>此时，多数Accepter知道在<em>某一个时刻</em>前某个提案（AcceptedProposeId）是生效的。其他Proposer了解到多数Accepter都有AcceptedProposeId，则其不能获得租约。</p>

<p>这里说明一下上句中的<em>某一时刻</em>。借用参考[1]中的图</p>

<p><img src="http://ikarishinjieva.github.com/blog/images/2013-12-19-paxos-lease-2.png"></p>

<p>可以看到Proposer和Accepter间有时间差，即<em>某一个时刻</em>指的是当前Accepter定时器超时的时刻（可能晚于Proposer上租约到期的时刻），但这并没有影响<em>不变式</em>成立。即在满足当前不变式时，<strong>不要求各个节点时钟同步</strong></p>

<hr />

<p>以上是我个人的理解，如有不妥，烦请看官comment</p>

<p>建议此时参看文末的参考文献和Keyspace源码。之后请期待下一篇：实现PaxosLease中的一些问题和解决</p>

<p>顺便吐个槽，我没有数学天赋和算法天赋，也实在没兴趣下苦工，实在不够进取。罪过罪过。</p>

<hr />

<h2>参考文献</h2>

<p>[1]<a href="http://dsdoc.net/paxoslease/index.html">【译】PaxosLease：实现租约的无盘Paxos算法</a></p>

<p>[2] <a href="https://github.com/scalien/keyspace/tree/master/src/Framework/PaxosLease">Keyspace源码</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于DEV测试的一些经验总结]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/12/09/test-framework-exp/"/>
    <updated>2013-12-09T20:05:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/12/09/test-framework-exp</id>
    <content type="html"><![CDATA[<p>已经是第三遍重写项目主代码了，这几天用主代码和测试框架互相补充。总结下经验值，以期升级</p>

<hr />

<p>关于测试框架，重写了几遍仍然保留下来的功重要能是：</p>

<ol>
<li><p>插桩代码，后面的功能也都依赖插桩。不在主代码中插桩，测试基本靠拜神；</p></li>
<li><p>条件池，根据条件池，代码才能沿着需要的分支进行；</p></li>
<li><p>签到点（checkpoint），代码要能根据测试要求停得下来，等待状态，之后跑得起来；</p></li>
<li><p>外缘测试。比起直接测试变量，还是分析log比较容易维护</p></li>
</ol>


<p>测试的难点是：</p>

<ol>
<li><p>资源回收。要连续跑测试，资源回收是说说容易的事。tcp server关了，listener停了，关闭的那些connection会不会瞬时占用临时端口；如果某一个connection正在申请二步锁，测试停止时远端锁失败，近端锁如果不释放会不会影响之后的测试；如果系统会自动重启tcp server，tcp server是在测试关闭操作之前停还是之后停还是停两次。想想头就大了。</p></li>
<li><p>想停都停不下来，万一断言失败，测试要能停下来。整个flow上都要处理停止中断，用“硬”中断很难掌握资源回收的状况；用“模拟”中断，所有等待/超时/重试的地方都要处理，逐级退栈。比起不测试的代码，主代码花在错误处理的代码量要大很多，但是值得。</p></li>
<li><p>测试界限把握不易。测粗了没作用，测细了耗时间而且波动大。插桩的深度，模拟中断的层级，这些都要拿好轻重。否则代码已腐败。</p></li>
</ol>


<p>每次跑测试，比起旁边坐个QA的测试，更步步惊心。QA一天才跑十几个case，自动的话2-3分钟就跑十几个简化的case，出错的概率要大很多。</p>

<p>最后反思一下，如果靠人肉测，&#8230;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对Memory Reordering Caught in the Act的学习 续 - 关于go的部分]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/11/11/study-memory-reorder-cont/"/>
    <updated>2013-11-11T20:44:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/11/11/study-memory-reorder-cont</id>
    <content type="html"><![CDATA[<p>这篇主要解决<a href="http://ikarishinjieva.github.io/blog/blog/2013/11/07/study-memory-reorder/">上一篇</a>遗留下来的问题，问题的简要描述请参看<a href="http://stackoverflow.com/questions/19901615/why-go-doesnt-show-memory-reordering">我发在SO上的帖子</a></p>

<p>主要的问题是用c++可以重现memory reordering，但go的程序没有重现</p>

<p>主要的结论是写go的时候我忘记设置GOMAXPROC，在目前这个go版本(1.2 rc2)下，不设置GOMAXPROC goroutine不会并发的，自然也没法设置memory reordering</p>

<p>此篇主要内容到此结束，以下是这两天的一些探索过程和技巧，觉得还是挺有意思的</p>

<hr />

<h4>go tool生成的汇编码和真实的汇编码是有很大差距的</h4>

<p>这个结论并不奇怪，但是差异的程度还是会影响诸如lock-free的代码的使用前提</p>

<p>对以下代码做对比</p>

<pre><code>x = 1
r1 = y
</code></pre>

<p>使用<code>go tool 6g -S xxx.go</code>反编译后的代码</p>

<pre><code>0246 (a.go:25) MOVQ    $1,x+0(SB)   //X=1
0247 (a.go:26) MOVQ    y+0(SB),BX
0248 (a.go:26) MOVQ    BX,r1+0(SB)  //r1=Y
</code></pre>

<p>而真实运行在cpu上的代码（<code>ndisasm -b 32 xxx</code>)为</p>

<pre><code>000013EB  C70425787F170001  mov dword [0x177f78],0x1     //X=1
         -000000
000013F6  48                dec eax
000013F7  8B1C25807F1700    mov ebx,[0x177f80]
000013FE  48                dec eax
000013FF  891C25687F1700    mov [0x177f68],ebx          //r1=Y
00001406  48                dec eax
</code></pre>

<p>可以看到在访问共享内存的前后多出了<code>dec eax</code>作为margin，这个原因不明，也没有找到相应的资料</p>

<p>但总的来说<code>ndisasm</code>产生的汇编代码更方便于对go行为的理解</p>

<hr />

<h4>一个小技巧快速定位汇编码</h4>

<p>我对intel指令集和go的编译器知之甚少，读起汇编码来颇为费劲。</p>

<p>快速定位源码对应的汇编码的位置，比较方便的就是修改一个数值，比如x=1改为x=2，前后生成的汇编码diff一下，就可以大概确定位置了</p>

<hr />

<h4>替换c++生成文件的指令</h4>

<p>在探索过程中，我想做个对比实验来证明是否上面所说的<code>dec eax</code>引起了c++和go在memory reordering上的差异，于是就想将<code>dec eax</code>也加到c++的生成文件中，这样就可以对比效果</p>

<p>碰到的问题是如果我直接将<code>asm volatile("dec %eax")</code>直接加到c++源码中，生成的汇编代码不是<code>48</code>，而是<code>FExxxx</code>。翻看<a href="http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-2a-manual.pdf">Intel® 64 and IA-32 Architectures
Software Developer’s Manual</a>，可知<code>dec</code>有多种形式</p>

<p>但是我不想研究为什么编译器会选择<code>FExxxx</code>而不是<code>48</code>，而是想尽快将c++生成的汇编代码形式做成和go一样。于是就有了下面的步骤：</p>

<ol>
<li><code>48</code>有两个字节，我也选取两个字节的op写在c++源码中，比如<code>asm volatile("cli")</code></li>
<li>c++编译生成，然后用16进制编辑器将<code>cli</code>生成的两个字节换成<code>48</code>即可</li>
</ol>


<p>之所以选择替换是因为怕有checksum或者内存位置的偏移，我也不知道有还是没有&#8230;</p>

<p>对比实验证明<code>dec eax</code>不是引起差异的原因</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对Memory Reordering Caught in the Act的学习]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/11/07/study-memory-reorder/"/>
    <updated>2013-11-07T21:40:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/11/07/study-memory-reorder</id>
    <content type="html"><![CDATA[<p>最近迷上了preshing.com，真的是非常专业的blog，每篇深浅合适而且可以相互印证，达到出书的质量了</p>

<p>学习了<a href="http://preshing.com/20120515/memory-reordering-caught-in-the-act/">Memory Reordering Caught in the Act</a>，内容很简单，主要是说“即使汇编码是顺序的，CPU执行时会对Load-Save进行乱序执行，导致无锁的两线程出现意料之外的结果”</p>

<p>简述一下：</p>

<ul>
<li>首先我们有两个线程，Ta和Tb，且有四个公共变量，a,b,r1,r2</li>
<li>Ta的代码是 a=1, r1=b</li>
<li>Tb的代码是 b=1, r2=a</li>
<li>保证编译器不做乱序优化</li>
<li>由于两个线程的读都在写之后，那么理论上，r1和r2中至少有一个应为1，或者都为1</li>
<li>但实际并非如此</li>
</ul>


<p>原因是CPU会做乱序执行，因为Ta/Tb的代码乱序后，比如r1=b, a=1，从单线程的角度来看对结果没有影响。而对于多线程，就会出现r1=r2=0的状况</p>

<p>解决方案是在两句之间插入Load-Save fence，参看<a href="http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/">这里</a></p>

<p>我自己用go想重现这个场景，代码参看最后。但是奇怪的是go的编译码跟文章描述的差不多</p>

<pre><code>[thread 1]
...
MOVQ    $1,a+0(SB)
MOVQ    b+0(SB),BX
MOVQ    BX,r1+0(SB)

[thread 2]
MOVQ    $1,b+0(SB)
MOVQ    a+0(SB),BX
MOVQ    BX,r2+0(SB)
</code></pre>

<p>但是在MBP (Intel Core i7)上跑并没有出现CPU乱序的现象，希望有同学能帮我提供线索，谢谢</p>

<p>(2013.11.11 更新：关于以上现象的原因参看<a href="http://ikarishinjieva.github.io/blog/blog/2013/11/11/study-memory-reorder-cont/">续 - 关于go的部分</a>)</p>

<p>go 代码：</p>

<pre><code>package main

import (
    "fmt"
    "math/rand"
)

var x, y, r1, r2 int
var detected = 0

func randWait() {
    for rand.Intn(8) != 0 {
    }
}

func main() {
    beginSig1 := make(chan bool, 1)
    beginSig2 := make(chan bool, 1)
    endSig1 := make(chan bool, 1)
    endSig2 := make(chan bool, 1)
    go func() {
        for {
            &lt;-beginSig1
            randWait()
            x = 1
            r1 = y
            endSig1 &lt;- true
        }
    }()
    go func() {
        for {
            &lt;-beginSig2
            randWait()
            y = 1
            r2 = x
            endSig2 &lt;- true
        }
    }()
    for i := 1; ; i = i + 1 {
        x = 0
        y = 0
        beginSig1 &lt;- true
        beginSig2 &lt;- true
        &lt;-endSig1
        &lt;-endSig2
        if r1 == 0 &amp;&amp; r2 == 0 {
            detected = detected + 1
            fmt.Println(detected, "reorders detected after ", i, "iterations")
        }
    }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对heartbeat φ累积失败检测算法的学习]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/11/05/accrual-failure-detector/"/>
    <updated>2013-11-05T21:50:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/11/05/accrual-failure-detector</id>
    <content type="html"><![CDATA[<p>偶尔读到了这篇&#8221;<a href="http://blog.csdn.net/chen77716/article/details/6541968">φ累积失败检测算法</a>&#8220;，写的非常不错。藉此了解了这个用于heartbeat检测的算法，在此记录一下我自己理解的简单版本</p>

<p>heartbeat时我们使用固定的时间限制t0，当heartbeat的返回时长超过t0时，就认为heartbeat失败。这个方法的弊端是：固定的t0是在事先测定的，不会随网络状况的变化而智能变化。φ累积失败检测算法就是要解决这个问题</p>

<p>失败检验算法的基本思想就是：成功判定“heartbeat失败”的概率符合<a href="http://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83">正态分布曲线</a>，x轴是本次心跳距上次心跳的差距时间，y轴是差距为x的心跳的概率。</p>

<br/>也就是说，假设我们已经有一条正态分布的曲线，当前时间是Tnow，上次心跳成功的时间是Tlast，那么从(Tlast-Tnow) ~ +∞这个区间内的积分（设为w，w<1）就代表某心跳间隔从Tlast维持到大于Tnow的时间的概率，即在Tnow时判定“heartbeat失败”的<b>失败率</b>，就是说如果我们在Tnow这个时间点判定“heartbeat失败”，那么有w的概率我们做出了错误的判定（heartbeat本该是成功的，也许只是被延迟了= =）


<p>臆测这个算法的基本步骤是：</p>

<ol>
<li>我们假设判定失败率的阈值是&lt;=10%，也就是允许我们判定“heartbeat失败”时最大失败率为10%。</li>
<li>取样本空间，比如前N次心跳的差距时间（心跳接收时间-上次心跳的接收时间）。计算这个样本空间的均值和方差，就可以计算出正态分布曲线</li>
<li>在某时间Tnow，计算(Tlast-Tnow) ~ +∞这个区间内的积分（设为w），即为判定“heartbeat失败”的<b>失败率</b>，若大于阈值10%，则可以判定“heartbeat”失败</li>
<li>重复取样，继续算法</li>
</ol>


<p>到此基本结束，以下是对原文&#8221;<a href="http://blog.csdn.net/chen77716/article/details/6541968">φ累积失败检测算法</a>&#8220;的一些个人补充</p>

<ul>
<li>原文有φ这个变量，主要是因为计算出来的判定失败率可能经常是非常小的小数，所以φ取其负对数，方便比较</li>
<li>在此不再重复引用原文的公式</li>
</ul>


<p>最后，可参考论文<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDEQFjAA&amp;url=http%3A%2F%2Fddg.jaist.ac.jp%2Fpub%2FHDY%2B04.pdf&amp;ei=L_94Uo3OGomciQLCx4GQBg&amp;usg=AFQjCNGYrM_1R5LmY4wrDlKnykatr3VBRA&amp;sig2=G8d5gBsR8MpIwgfU9Xbt7A&amp;bvm=bv.55980276,d.cGE">
The φ Accrual Failure Detector</a>：</p>

<ul>
<li>这篇论文非常详细（啰嗦）地描述了要解决的问题场景</li>
<li>这篇论文给出了一般性的累积失败检测法要满足的特性</li>
<li>这篇论文给出了用正态分布曲线来计算的步骤</li>
<li>这篇论文给出了算法正确性的比较结果</li>
</ul>


<p>最后的最后，推荐<a href="http://blog.csdn.net/chen77716">这个大牛陈国庆的blog</a>，其中文章写的质量高，里面也有对Paxos算法的介绍，配合paxos的wiki，解析的很到位</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对Mysql bug #70307 的学习]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/10/25/study-mysql-bug-70307/"/>
    <updated>2013-10-25T22:00:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/10/25/study-mysql-bug-70307</id>
    <content type="html"><![CDATA[<p>之前描述<a href="http://ikarishinjieva.github.io/blog/blog/2013/10/11/hole-in-mysql-56-replication-dead-lock/">Mysql 5.6.15 Replication中碰到的死锁</a>的情况，这次尝试debug下原因。</p>

<h2>debug的过程</h2>

<p>用参数&#8211;gdb启动mysql，按照<a href="http://bugs.mysql.com/file.php?id=20542">步骤</a>重现bug（让slave &#8220;show slave status&#8221;时卡住）。然后用gdb attach到slave mysql实例上。</p>

<pre><code>(gdb) thread apply all bt
</code></pre>

<p>输出所有线程的backtrace，找到show slave status卡住的线程和位置</p>

<pre><code>Thread 2 (Thread 0x7f583c166700 (LWP 2440)):
#0  0x00007f583f484054 in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x00007f583f47f3be in _L_lock_995 () from /lib64/libpthread.so.0
#2  0x00007f583f47f326 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000aa3cde in safe_mutex_lock (mp=0x3516ae8, try_lock=0 '\000', file=0xfb8e58 "/home/vagrant/mysql-5.6.12/sql/rpl_slave.cc", line=2611) at /home/vagrant/mysql-5.6.12/mysys/thr_mutex.c:152
#4  0x0000000000a4b993 in inline_mysql_mutex_lock (that=0x3516ae8, src_file=0xfb8e58 "/home/vagrant/mysql-5.6.12/sql/rpl_slave.cc", src_line=2611) at /home/vagrant/mysql-5.6.12/include/mysql/psi/mysql_thread.h:686
#5  0x0000000000a53cb3 in show_slave_status (thd=0x352e3d0, mi=0x34b4f20) at /home/vagrant/mysql-5.6.12/sql/rpl_slave.cc:2611
#6  0x00000000007d45f4 in mysql_execute_command (thd=0x352e3d0) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:2766
#7  0x00000000007ddc46 in mysql_parse (thd=0x352e3d0, rawbuf=0x7f57ec005010 "show slave status", length=17, parser_state=0x7f583c165660) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:6187
#8  0x00000000007d1019 in dispatch_command (command=COM_QUERY, thd=0x352e3d0, packet=0x3534e51 "", packet_length=17) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:1334
#9  0x00000000007d017b in do_command (thd=0x352e3d0) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:1036
#10 0x0000000000797a08 in do_handle_one_connection (thd_arg=0x352e3d0) at /home/vagrant/mysql-5.6.12/sql/sql_connect.cc:977
#11 0x00000000007974e4 in handle_one_connection (arg=0x352e3d0) at /home/vagrant/mysql-5.6.12/sql/sql_connect.cc:893
#12 0x0000000000aea87a in pfs_spawn_thread (arg=0x351b510) at /home/vagrant/mysql-5.6.12/storage/perfschema/pfs.cc:1855
#13 0x00007f583f47d851 in start_thread () from /lib64/libpthread.so.0
#14 0x00007f583e3e890d in clone () from /lib64/libc.so.6
</code></pre>

<p>可以看到show slave status卡在</p>

<pre><code>#5  0x0000000000a53cb3 in show_slave_status (thd=0x352e3d0, mi=0x34b4f20) at /home/vagrant/mysql-5.6.12/sql/rpl_slave.cc:2611
</code></pre>

<p>查找源码可以看到show slave status卡在获取锁mi->rli->data_lock上<br/>(科普下缩写: mi=master info, rli=relay log info</p>

<p>在gdb中运行命令</p>

<pre><code>(gdb) thread 2
(gdb) f 5
(gdb) print mi-&gt;rli-&gt;data_lock
</code></pre>

<p>切换到thread 2堆栈第5层的上下文，打印出mi->rli->data_lock变量，输出如下</p>

<pre><code>$1 = {m_mutex = {global = {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 0, __kind = 2, __spins = 0,
    __list = {__prev = 0x0, __next = 0x0}},
  __size = '\000' &lt;repeats 16 times&gt;, "\002", '\000' &lt;repeats 22 times&gt;, __align = 0}, mutex = {__data = {
    __lock = 2, __count = 0, __owner = 2435, __nusers = 1, __kind = 3, __spins = 0, __list = {__prev = 0x0,
      __next = 0x0}},
  __size = "\002\000\000\000\000\000\000\000\203\t\000\000\001\000\000\000\003", '\000' &lt;repeats 22 times&gt;,
  __align = 2}, file = 0xfa4520 "/home/vagrant/mysql-5.6.12/sql/log_event.cc", line = 7259, count = 1,
thread = 140016942216960}, m_psi = 0x0}
</code></pre>

<p>看到锁的owner是线程(LWP 2435)，为Thread 3</p>

<p>Thread 3的backtrace如下</p>

<pre><code>Thread 3 (Thread 0x7f583c1a7700 (LWP 2435)):
#0  0x00007f583f4817bb in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x0000000000aa429d in safe_cond_timedwait (cond=0x7f57f4000ba8, mp=0x7f57f4000b38, abstime=0x7f583c1a60f0, file=0xedc960 "/home/vagrant/mysql-5.6.12/include/mysql/psi/mysql_thread.h", line=1199) at /home/vagrant/mysql-5.6.12/mysys/thr_mutex.c:278
#2  0x00000000007121f4 in inline_mysql_cond_timedwait (that=0x7f57f4000ba8, mutex=0x7f57f4000b38, abstime=0x7f583c1a60f0, src_file=0xedcb98 "/home/vagrant/mysql-5.6.12/sql/mdl.cc", src_line=1306) at /home/vagrant/mysql-5.6.12/include/mysql/psi/mysql_thread.h:1199
#3  0x0000000000713111 in MDL_wait::timed_wait (this=0x7f57f4000b38, owner=0x7f57f4000a50, abs_timeout=0x7f583c1a60f0, set_status_on_timeout=true, wait_state_name=0x14d0488) at /home/vagrant/mysql-5.6.12/sql/mdl.cc:1306
#4  0x0000000000714811 in MDL_context::acquire_lock (this=0x7f57f4000b38, mdl_request=0x7f583c1a6180, lock_wait_timeout=31536000) at /home/vagrant/mysql-5.6.12/sql/mdl.cc:2241
#5  0x000000000063656a in ha_commit_trans (thd=0x7f57f4000a50, all=true) at /home/vagrant/mysql-5.6.12/sql/handler.cc:1396 (COMMIT LOCK)
#6  0x00000000008a010b in trans_commit (thd=0x7f57f4000a50) at /home/vagrant/mysql-5.6.12/sql/transaction.cc:228
#7  0x0000000000a081bb in Xid_log_event::do_commit (this=0x7f57f4004730, thd=0x7f57f4000a50) at /home/vagrant/mysql-5.6.12/sql/log_event.cc:7174
#8  0x0000000000a0886e in Xid_log_event::do_apply_event (this=0x7f57f4004730, rli=0x3516650) at /home/vagrant/mysql-5.6.12/sql/log_event.cc:7310 (rli-&gt;data_lock)
#9  0x00000000009fd956 in Log_event::apply_event (this=0x7f57f4004730, rli=0x3516650) at /home/vagrant/mysql-5.6.12/sql/log_event.cc:3049
#10 0x0000000000a55e31 in apply_event_and_update_pos (ptr_ev=0x7f583c1a68a0, thd=0x7f57f4000a50, rli=0x3516650) at /home/vagrant/mysql-5.6.12/sql/rpl_slave.cc:3374
#11 0x0000000000a56e45 in exec_relay_log_event (thd=0x7f57f4000a50, rli=0x3516650) at /home/vagrant/mysql-5.6.12/sql/rpl_slave.cc:3742
#12 0x0000000000a5c334 in handle_slave_sql (arg=0x34b4f20) at /home/vagrant/mysql-5.6.12/sql/rpl_slave.cc:5552
#13 0x0000000000aea87a in pfs_spawn_thread (arg=0x350a800) at /home/vagrant/mysql-5.6.12/storage/perfschema/pfs.cc:1855
#14 0x00007f583f47d851 in start_thread () from /lib64/libpthread.so.0
#15 0x00007f583e3e890d in clone () from /lib64/libc.so.6
</code></pre>

<p>可以看到Thread 3卡在commit lock上，同时查源码看到Thread 3同时占有了rli->data_lock (log_event.cc:7259)</p>

<h2>锁的状态</h2>

<p>按照bug的描述，</p>

<ol>
<li>flush tables with read lock; 会持有commit lock</li>
<li>IO thread (Thread 3)会持有rli->data_lock，并等待commit lock</li>
<li>show slave status; 会等待rli->data_lock</li>
</ol>


<p>结果导致show slave status卡住不可用</p>

<h2>臆测一下解决方法</h2>

<p>鉴于功底不深，只能臆测一下</p>

<ol>
<li>IO thread持有锁rli->data_lock的原因是要更新relay log的状态，然后进行commit(Xid_log_event::do_apply_event (log_event.cc:7248))。在commit的时候不会更新rli的数据。</li>
<li>show slave status不会更新rli的数据，需要锁rli->data_lock的原因是要一致性数据。</li>
</ol>


<p>因此可能的解决方案是IO thread持有读写锁，进行commit时转为持有读锁。show slave status只使用读锁。</p>

<p>只是臆测下解决方法，待<a href="http://bugs.mysql.com/bug.php?id=70307">bug #70307</a>修掉时再学习。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql 5.6.12 master上flush logs在slave上产生两个relay-log]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/10/22/mysql-flush-logs-make-two-relay-log-file/"/>
    <updated>2013-10-22T21:42:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/10/22/mysql-flush-logs-make-two-relay-log-file</id>
    <content type="html"><![CDATA[<h2>现象</h2>

<p>一个碰巧观察到的有趣的现象：mysql 5.6.12 在master上flush logs，在slave上会观察到两个新的relay-log file</p>

<p>举例：</p>

<p>slave-relay-bin.000092</p>

<pre><code> FD event
 Rotate to mysql-bin.000056
 Rotate to slave-relay-bin.000093
</code></pre>

<p>slave-relay-bin.000093</p>

<pre><code> FD event slave
 Rotate to mysql-bin.000056
 FD event master
 bla bla…
</code></pre>

<p>可以看到000092这个relay log相当多余。这个现象并不会影响replication的正确性，只是让有强迫症的人有点狂躁</p>

<h2>探索</h2>

<p>在master上net_serv.cc:my_net_write打断点，可以观察到master的确发出了以下三个事件</p>

<ul>
<li>ROTATE_EVENT</li>
</ul>


<p>backtrace</p>

<pre><code>#0  my_net_write (net=0x1ea2858, packet=0x7fffa4002b70 "", len=48)
    at /home/vagrant/mysql-5.6.12/sql/net_serv.cc:284
#1  0x0000000000a48b05 in mysql_binlog_send (thd=0x1ea2600, log_ident=0x7fffa4004c60 "mysql-bin.000052", pos=167,
    slave_gtid_executed=0x0) at /home/vagrant/mysql-5.6.12/sql/rpl_master.cc:1336
#2  0x0000000000a46ad2 in com_binlog_dump (thd=0x1ea2600, packet=0x1ea5d21 "", packet_length=26)
    at /home/vagrant/mysql-5.6.12/sql/rpl_master.cc:746
#3  0x00000000007d1ab9 in dispatch_command (command=COM_BINLOG_DUMP, thd=0x1ea2600, packet=0x1ea5d21 "",
    packet_length=26) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:1534
#4  0x00000000007d017b in do_command (thd=0x1ea2600) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:1036
#5  0x0000000000797a08 in do_handle_one_connection (thd_arg=0x1ea2600)
    at /home/vagrant/mysql-5.6.12/sql/sql_connect.cc:977
#6  0x00000000007974e4 in handle_one_connection (arg=0x1ea2600)
    at /home/vagrant/mysql-5.6.12/sql/sql_connect.cc:893
#7  0x0000000000aea87a in pfs_spawn_thread (arg=0x1e7aa80)
    at /home/vagrant/mysql-5.6.12/storage/perfschema/pfs.cc:1855
#8  0x00007ffff7bc7851 in start_thread () from /lib64/libpthread.so.0
#9  0x00007ffff6b3290d in clone () from /lib64/libc.so.6
</code></pre>

<ul>
<li>第二个ROTATE_EVENT</li>
</ul>


<p>backtrace</p>

<pre><code>#0  my_net_write (net=0x1ea2858, packet=0x7fffa4002ab0 "", len=48)
    at /home/vagrant/mysql-5.6.12/sql/net_serv.cc:284
#1  0x0000000000a45f04 in fake_rotate_event (net=0x1ea2858, packet=0x1ea2be8,
    log_file_name=0x7fffc94ff270 "./mysql-bin.000056", position=4, errmsg=0x7fffc94ffdb0,
    checksum_alg_arg=1 '\001') at /home/vagrant/mysql-5.6.12/sql/rpl_master.cc:395
#2  0x0000000000a4a33d in mysql_binlog_send (thd=0x1ea2600, log_ident=0x7fffa4004c60 "mysql-bin.000052", pos=167,
    slave_gtid_executed=0x0) at /home/vagrant/mysql-5.6.12/sql/rpl_master.cc:1728
#3  0x0000000000a46ad2 in com_binlog_dump (thd=0x1ea2600, packet=0x1ea5d21 "", packet_length=26)
    at /home/vagrant/mysql-5.6.12/sql/rpl_master.cc:746
#4  0x00000000007d1ab9 in dispatch_command (command=COM_BINLOG_DUMP, thd=0x1ea2600, packet=0x1ea5d21 "",
    packet_length=26) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:1534
#5  0x00000000007d017b in do_command (thd=0x1ea2600) at /home/vagrant/mysql-5.6.12/sql/sql_parse.cc:1036
#6  0x0000000000797a08 in do_handle_one_connection (thd_arg=0x1ea2600)
    at /home/vagrant/mysql-5.6.12/sql/sql_connect.cc:977
#7  0x00000000007974e4 in handle_one_connection (arg=0x1ea2600)
    at /home/vagrant/mysql-5.6.12/sql/sql_connect.cc:893
#8  0x0000000000aea87a in pfs_spawn_thread (arg=0x1e7aa80)
    at /home/vagrant/mysql-5.6.12/storage/perfschema/pfs.cc:1855
#9  0x00007ffff7bc7851 in start_thread () from /lib64/libpthread.so.0
#10 0x00007ffff6b3290d in clone () from /lib64/libc.so.6
</code></pre>

<ul>
<li>FORMAT_DESCRIPTION_EVENT</li>
</ul>


<p>可以看到第一个ROTATE_EVENT是由flush logs发出的，第二个ROTATE_EVENT是fake_rotate_event</p>

<h2>关于fake_rotate_event</h2>

<p>以前也<a href="http://ikarishinjieva.github.io/blog/blog/2013/10/16/mysql-mysql_binlog_send-src/">吐槽</a>过fake_rotate_event</p>

<p>master在binlog切换时（不一定是手工flush，也可能是重启，或者容量达到限制）一定要多发一个rotate event，原因如源码rpl_master.cc:mysql_binlog_send中的注释</p>

<pre><code>  /*
    Call fake_rotate_event() in case the previous log (the one which
    we have just finished reading) did not contain a Rotate event.
    There are at least two cases when this can happen:

    - The previous binary log was the last one before the master was
      shutdown and restarted.

    - The previous binary log was GTID-free (did not contain a
      Previous_gtids_log_event) and the slave is connecting using
      the GTID protocol.

    This way we tell the slave about the new log's name and
    position.  If the binlog is 5.0 or later, the next event we
    are going to read and send is Format_description_log_event.
  */
  if ((file=open_binlog_file(&amp;log, log_file_name, &amp;errmsg)) &lt; 0 ||
      fake_rotate_event(net, packet, log_file_name, BIN_LOG_HEADER_SIZE,
                        &amp;errmsg, current_checksum_alg))
</code></pre>

<p>主要是解决之前没有rotate event发送的场景</p>

<p>虽然非常想吐槽，但是我也想不出更好的办法</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql rpl_slave.cc:handle_slave_io 源码的一些个人分析]]></title>
    <link href="http://ikarishinjieva.github.com/blog/blog/2013/10/20/mysql-handle_slave_io-src/"/>
    <updated>2013-10-20T20:17:00+08:00</updated>
    <id>http://ikarishinjieva.github.com/blog/blog/2013/10/20/mysql-handle_slave_io-src</id>
    <content type="html"><![CDATA[<p>读了rpl_slave.cc:handle_slave_io的源码（Mysql 5.6.11），总结一下</p>

<h2>函数概述</h2>

<p>handle_slave_io是slave io_thread的主函数，函数逻辑入口为rpl_slave.cc:start_slave_threads</p>

<h2>主体结构</h2>

<figure class='code'><figcaption><span>源码的主体结构  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">handle_slave_io</span><span class="o">(</span><span class="n">master_info</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>     <span class="mi">3955</span> <span class="n">bla</span> <span class="n">bla</span><span class="err">…</span>
</span><span class='line'>     <span class="mi">4016</span> <span class="n">fire</span> <span class="n">HOOK</span> <span class="n">binlog_relay_io</span><span class="o">.</span><span class="na">thread_start</span>
</span><span class='line'>     <span class="mi">4032</span> <span class="err">与</span><span class="n">master</span><span class="err">建立连接</span>
</span><span class='line'>    <span class="o">(</span><span class="mi">4047</span> <span class="err">设置</span><span class="n">max_packet_size</span><span class="o">)</span>
</span><span class='line'>     <span class="mi">4073</span> <span class="n">get_master_version_and_clock</span><span class="o">,</span>
</span><span class='line'>          <span class="err">在</span><span class="n">master</span><span class="err">上：</span>
</span><span class='line'>          <span class="err">通过</span><span class="n">SELECT</span> <span class="n">UNIX_TIMESTAMP</span><span class="o">()</span><span class="err">获取</span><span class="n">server</span> <span class="n">timestamp</span>
</span><span class='line'>          <span class="err">通过</span><span class="n">SHOW</span> <span class="n">VARIABLES</span> <span class="n">LIKE</span> <span class="err">&#39;</span><span class="n">SERVER_ID</span><span class="err">&#39;获取</span><span class="n">server</span> <span class="n">id</span>
</span><span class='line'>          <span class="n">SET</span> <span class="nd">@master_heartbeat_period</span><span class="o">=</span> <span class="o">?</span>
</span><span class='line'>          <span class="n">SET</span> <span class="nd">@master_binlog_checksum</span><span class="o">=</span> <span class="err">@</span><span class="nd">@global.binlog_checksum</span>
</span><span class='line'>          <span class="n">SELECT</span> <span class="nd">@master_binlog_checksum</span><span class="err">获取</span><span class="n">master</span> <span class="n">binlog</span> <span class="n">checksum</span>
</span><span class='line'>          <span class="n">SELECT</span> <span class="err">@</span><span class="nd">@GLOBAL.GTID_MODE</span>
</span><span class='line'>     <span class="mi">4075</span> <span class="n">get_master_uuid</span>
</span><span class='line'>          <span class="err">在</span><span class="n">master</span><span class="err">上“</span><span class="n">SHOW</span> <span class="n">VARIABLES</span> <span class="n">LIKE</span> <span class="err">&#39;</span><span class="n">SERVER_UUID</span><span class="err">&#39;”</span>
</span><span class='line'>     <span class="mi">4077</span> <span class="n">io_thread_init_commands</span>
</span><span class='line'>          <span class="err">在</span><span class="n">master</span><span class="err">上“</span><span class="n">SET</span> <span class="nd">@slave_uuid</span><span class="o">=</span> <span class="err">&#39;</span><span class="o">%</span><span class="n">s</span><span class="err">&#39;”</span>
</span><span class='line'>     <span class="mi">4106</span> <span class="n">register_slave_on_master</span>
</span><span class='line'>          <span class="err">向</span><span class="n">master</span><span class="err">发送</span><span class="n">COM_REGISTER_SLAVE</span>
</span><span class='line'>     <span class="mi">4133</span> <span class="k">while</span> <span class="o">(!</span><span class="n">io_slave_killed</span><span class="o">(</span><span class="n">thd</span><span class="o">,</span><span class="n">mi</span><span class="o">))</span>
</span><span class='line'>     <span class="mi">4134</span> <span class="o">{</span>
</span><span class='line'>     <span class="mi">4136</span>      <span class="n">request_dump</span>
</span><span class='line'>               <span class="err">向</span><span class="n">master</span><span class="err">发送</span><span class="n">COM_BINLOG_DUMP_GTID</span><span class="o">/</span><span class="n">COM_BINLOG_DUMP</span>
</span><span class='line'>     <span class="mi">4159</span>      <span class="k">while</span> <span class="o">(!</span><span class="n">io_slave_killed</span><span class="o">(</span><span class="n">thd</span><span class="o">,</span><span class="n">mi</span><span class="o">))</span>
</span><span class='line'>     <span class="mi">4160</span>      <span class="o">{</span>
</span><span class='line'>     <span class="mi">4169</span>           <span class="n">read_event</span><span class="err">，此为阻塞方法，会阻塞等待有新数据包传入</span>
</span><span class='line'>     <span class="mi">4184</span>          <span class="o">{</span>
</span><span class='line'>                         <span class="err">一些包错误的处理，包括</span><span class="n">packet</span> <span class="n">too</span> <span class="n">large</span> <span class="o">/</span> <span class="n">out</span> <span class="n">of</span> <span class="n">resource</span><span class="err">等</span>
</span><span class='line'>     <span class="mi">4213</span>          <span class="o">}</span>
</span><span class='line'>     <span class="mi">4219</span>          <span class="n">fire</span> <span class="n">HOOK</span> <span class="n">binlog_relay_io</span><span class="o">.</span><span class="na">after_read_event</span>
</span><span class='line'>     <span class="mi">4232</span>          <span class="n">queue_event</span><span class="err">，将</span><span class="n">event</span><span class="err">放入</span><span class="n">relay</span> <span class="n">log</span><span class="err">写</span><span class="n">buf</span>
</span><span class='line'>     <span class="mi">4240</span>          <span class="n">fire</span> <span class="n">HOOK</span> <span class="n">binlog_relay_io</span><span class="o">.</span><span class="na">after_queue_event</span>
</span><span class='line'>     <span class="mi">4250</span>          <span class="n">flush_master_info</span><span class="err">，将</span><span class="n">master_info</span><span class="err">和</span><span class="n">relay</span> <span class="n">log</span><span class="err">刷到</span><span class="n">disk</span><span class="err">上</span>
</span><span class='line'>                   <span class="err">此处，先刷</span><span class="n">relay</span> <span class="n">log</span><span class="err">，后刷</span><span class="n">master_info</span><span class="err">。这样意外的故障可以通过重连恢复机制来恢复。</span>
</span><span class='line'>                   <span class="err">若先刷</span><span class="n">master_info</span><span class="err">，后刷</span><span class="n">relay</span> <span class="n">log</span><span class="err">，意外故障时</span><span class="n">master_info</span><span class="err">已经更新，比如</span><span class="o">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">100</span><span class="o">,</span> <span class="mi">100</span><span class="o">-</span><span class="mi">200</span><span class="o">)</span><span class="err">，而数据丢失，仅有</span><span class="o">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">100</span><span class="o">)</span><span class="err">，恢复的</span><span class="n">replication</span><span class="err">会从</span><span class="mi">200</span><span class="err">开始。整个</span><span class="n">relay</span> <span class="n">log</span><span class="err">会成为</span><span class="o">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">100</span><span class="o">,</span> <span class="mi">200</span><span class="o">-)</span><span class="err">，中间数据会丢失。</span>
</span><span class='line'>
</span><span class='line'>     <span class="mi">4286</span>          <span class="err">若</span><span class="n">relay</span> <span class="n">log</span><span class="err">达到容量限制，则</span><span class="n">wait_for_relay_log_space</span>
</span><span class='line'>     <span class="mi">4292</span>      <span class="o">}</span>
</span><span class='line'>     <span class="mi">4293</span> <span class="o">}</span>
</span><span class='line'>     <span class="mi">4296</span> <span class="err">之后都是收尾操作</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>一些重点</h2>

<ol>
<li>此处不分析锁什么的，因为看不懂</li>
<li>4047 设置max_packet_size的目的不明</li>
<li>4073 开始slave会向master直接发送一些sql，然后解析返回。而不是包装在某个包的某个字段里，用一些预定义的变量来传递结果。<br/>这种设计一下就觉得山寨起来。<br/>后经同事 @神仙 指点，mysql这样做貌似是为了兼容性，免得数据包格式被改来改去。<br/>（看到mysql里大量的兼容代码都拿来处理包结构的问题，最极品的可能是莫过于LOG_EVENT_MINIMAL_HEADER_LEN了）<br/>在对流量影响不大的情况下，直接用sql反复查询的确是个好的解决手法</li>
<li>4250 将master_info和relay log刷到disk上。<br/>先刷relay log，后刷master_info。这样意外的故障可以通过relay log恢复机制来恢复。<br/>若先刷master_info，后刷relay log，意外故障时master_info已经更新，比如(0-100, 100-200)，而数据(100-200)丢失，仅有(0-100)，恢复的replication会从200开始。整个relay log会成为(0-100, 200-)，中间数据会丢失。</li>
</ol>


<h2>start slave时slave向master发送的事件</h2>

<ul>
<li><p>SELECT UNIX_TIMESTAMP() (rpl_slave.cc:get_master_version_and_clock)</p></li>
<li> SHOW VARIABLES LIKE &#8216;SERVER_ID&#8217; (rpl_slave.cc:get_master_version_and_clock)</li>
<li> SET @master_heartbeat_period=? (rpl_slave.cc:get_master_version_and_clock)</li>
<li> SET @master_binlog_checksum= @@global.binlog_checksum (rpl_slave.cc:get_master_version_and_clock)</li>
<li> SELECT @master_binlog_checksum (rpl_slave.cc:get_master_version_and_clock)</li>
<li> SELECT @@GLOBAL.GTID_MODE (rpl_slave.cc:get_master_version_and_clock)</li>
<li><p> SHOW VARIABLES LIKE &#8216;SERVER_UUID&#8217; （rpl_slave.cc:get_master_uuid）</p></li>
<li><p> SET @slave_uuid= &#8216;%s&#8217;（rpl_slave.cc:io_thread_init_commands)</p></li>
<li> COM_REGISTER_SLAVE(rpl_slave.cc:register_slave_on_master)</li>
<li> COM_BINLOG_DUMP(rpl_slave.cc:request_dump)</li>
</ul>


<h2>master与slave的时间差</h2>

<p>可以看到slave获得master的时间方法就是直接下sql，完全忽略网络延迟等等等等，属于不精准的时间</p>

<p><a href="http://guduwhuzhe.iteye.com/blog/1901707">这篇文章</a>从源码级别分析了Seconds_Behind_Master的来源，也给出了备库延迟跳跃的原因。总的来说就是Seconds_Behind_Master不可信。</p>
]]></content>
  </entry>
  
</feed>
